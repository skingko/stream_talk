import { ref, reactive, computed, watch } from 'vue'
import { ElMessage } from 'element-plus'

// å¯¹è¯çŠ¶æ€æšä¸¾
const ConversationState = {
  IDLE: 'idle',                    // ç©ºé—²çŠ¶æ€
  LISTENING: 'listening',          // ç›‘å¬ç”¨æˆ·
  RECORDING: 'recording',          // å½•éŸ³ä¸­
  PROCESSING: 'processing',        // å¤„ç†ç”¨æˆ·è¾“å…¥
  SPEAKING: 'speaking',           // AIå›å¤ä¸­
  INTERRUPTED: 'interrupted',      // è¢«ä¸­æ–­
  WAITING: 'waiting'              // ç­‰å¾…ç”¨æˆ·ç»§ç»­
}

// å…¨å±€å¯¹è¯çŠ¶æ€
const conversationState = reactive({
  currentState: ConversationState.IDLE,
  isVoiceMode: false,
  isListening: false,
  isProcessing: false,
  isSpeaking: false,
  isInterrupted: false,
  
  // VADç›¸å…³
  vadEnabled: false,
  speechDetected: false,
  silenceDetected: false,
  
  // è½¬å½•ç›¸å…³
  realtimeTranscript: '',
  finalTranscript: '',
  
  // å¯¹è¯ç®¡ç†
  conversationId: null,
  turnCount: 0,
  lastUserSpeechTime: null,
  lastAISpeechTime: null,
  messages: [], // å¯¹è¯æ¶ˆæ¯å†å²
  
  // ä¸­æ–­å¤„ç†
  interruptionCount: 0,
  interruptionThreshold: 300, // 300mså†…æ£€æµ‹åˆ°è¯­éŸ³å³ä¸­æ–­
  
  // é”™è¯¯å¤„ç†
  error: null,
  
  // ç»Ÿè®¡ä¿¡æ¯
  stats: {
    totalConversations: 0,
    totalInterruptions: 0,
    successfulInterruptions: 0,
    averageResponseTime: 0
  }
})

// WebSocketè¿æ¥
let wsConnection = null
let reconnectAttempts = 0
const maxReconnectAttempts = 5
let heartbeatInterval = null
let connectionCheckInterval = null

// éŸ³é¢‘ç›¸å…³
let audioContext = null
let mediaRecorder = null
let audioStream = null
let vadProcessor = null

export function useVoiceConversation() {
  
  /**
   * åˆå§‹åŒ–è¯­éŸ³å¯¹è¯ç³»ç»Ÿ
   */
  const initVoiceConversation = async () => {
    try {
      console.log('ğŸš€ åˆå§‹åŒ–è¯­éŸ³å¯¹è¯ç³»ç»Ÿ...')
      
      // 1. åˆå§‹åŒ–WebSocketè¿æ¥
      await initWebSocketConnection()
      
      // 2. åˆå§‹åŒ–éŸ³é¢‘ç³»ç»Ÿ
      await initAudioSystem()
      
      // 3. åˆå§‹åŒ–VAD
      await initVAD()
      
      console.log('âœ… è¯­éŸ³å¯¹è¯ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ')
      return true
      
    } catch (error) {
      console.error('âŒ è¯­éŸ³å¯¹è¯ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥:', error)
      conversationState.error = error.message
      return false
    }
  }
  
  /**
   * åˆå§‹åŒ–WebSocketè¿æ¥
   */
  const initWebSocketConnection = async () => {
    return new Promise((resolve, reject) => {
      try {
        const wsUrl = `ws://localhost:8002/ws/voice`
        wsConnection = new WebSocket(wsUrl)
        
        wsConnection.onopen = () => {
          console.log('ğŸ”— WebSocketè¿æ¥å·²å»ºç«‹')
          conversationState.error = null
          reconnectAttempts = 0

          // å‘é€åˆå§‹åŒ–æ¶ˆæ¯
          wsConnection.send(JSON.stringify({
            type: 'init',
            data: { client_type: 'vue_frontend' }
          }))

          // å¯åŠ¨å¿ƒè·³æœºåˆ¶
          startHeartbeat()

          // å¯åŠ¨è¿æ¥çŠ¶æ€æ£€æŸ¥
          startConnectionCheck()

          resolve()
        }
        
        wsConnection.onmessage = (event) => {
          handleWebSocketMessage(JSON.parse(event.data))
        }
        
        wsConnection.onclose = (event) => {
          console.log('ğŸ”Œ WebSocketè¿æ¥å·²å…³é—­:', event.code, event.reason)

          // æ¸…ç†å®šæ—¶å™¨
          stopHeartbeat()
          stopConnectionCheck()

          // åœ¨è¯­éŸ³æ¨¡å¼ä¸‹è‡ªåŠ¨é‡è¿ï¼Œæ— è®ºæ˜¯å¦æ­£å¸¸å…³é—­
          if (conversationState.isVoiceMode && reconnectAttempts < maxReconnectAttempts) {
            reconnectAttempts++
            console.log(`ğŸ”„ æ£€æµ‹åˆ°è¿æ¥æ–­å¼€ï¼Œè‡ªåŠ¨é‡è¿ (${reconnectAttempts}/${maxReconnectAttempts})...`)

            // æ˜¾ç¤ºé‡è¿æç¤º
            ElMessage.warning(`è¿æ¥æ–­å¼€ï¼Œæ­£åœ¨é‡è¿... (${reconnectAttempts}/${maxReconnectAttempts})`)

            const reconnectDelay = 1000 // 1ç§’åé‡è¿
            setTimeout(() => {
              initWebSocketConnection().then(() => {
                // é‡è¿æˆåŠŸåæ¢å¤è¯­éŸ³ç›‘å¬çŠ¶æ€
                console.log('ğŸ”„ é‡è¿æˆåŠŸï¼Œæ¢å¤è¯­éŸ³ç›‘å¬çŠ¶æ€')
                conversationState.currentState = ConversationState.LISTENING
                conversationState.vadEnabled = true
                ElMessage.success('è¿æ¥å·²æ¢å¤ï¼Œå¯ä»¥ç»§ç»­è¯­éŸ³å¯¹è¯')
                reconnectAttempts = 0 // é‡ç½®é‡è¿æ¬¡æ•°
              }).catch(error => {
                console.error('âŒ é‡è¿å¤±è´¥:', error)
                if (reconnectAttempts >= maxReconnectAttempts) {
                  ElMessage.error('è¿æ¥æ–­å¼€ï¼Œè¯·æ‰‹åŠ¨é‡æ–°å¼€å§‹å¯¹è¯')
                  conversationState.currentState = ConversationState.IDLE
                  conversationState.isVoiceMode = false
                }
              })
            }, reconnectDelay)
          } else if (!conversationState.isVoiceMode) {
            reconnectAttempts = 0
          }
        }
        
        wsConnection.onerror = (error) => {
          console.error('âŒ WebSocketé”™è¯¯:', error)
          reject(error)
        }
        
        // è¶…æ—¶å¤„ç†
        setTimeout(() => {
          if (wsConnection.readyState !== WebSocket.OPEN) {
            reject(new Error('WebSocketè¿æ¥è¶…æ—¶'))
          }
        }, 10000)
        
      } catch (error) {
        reject(error)
      }
    })
  }
  
  /**
   * å¯åŠ¨å¿ƒè·³æœºåˆ¶
   */
  const startHeartbeat = () => {
    // æ¯10ç§’å‘é€ä¸€æ¬¡å¿ƒè·³ï¼Œæ›´é¢‘ç¹ä»¥é˜²æ­¢è¶…æ—¶
    heartbeatInterval = setInterval(() => {
      if (wsConnection && wsConnection.readyState === WebSocket.OPEN) {
        wsConnection.send(JSON.stringify({
          type: 'ping',
          data: { timestamp: Date.now() }
        }))
        console.log('ğŸ’“ å‘é€å¿ƒè·³')
      }
    }, 10000)
  }

  /**
   * åœæ­¢å¿ƒè·³æœºåˆ¶
   */
  const stopHeartbeat = () => {
    if (heartbeatInterval) {
      clearInterval(heartbeatInterval)
      heartbeatInterval = null
      console.log('ğŸ’” åœæ­¢å¿ƒè·³')
    }
  }

  /**
   * å¯åŠ¨è¿æ¥çŠ¶æ€æ£€æŸ¥
   */
  const startConnectionCheck = () => {
    // æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡è¿æ¥çŠ¶æ€
    connectionCheckInterval = setInterval(() => {
      if (wsConnection && wsConnection.readyState !== WebSocket.OPEN) {
        console.warn('âš ï¸ WebSocketè¿æ¥å¼‚å¸¸ï¼ŒçŠ¶æ€:', wsConnection.readyState)

        // å¦‚æœè¿æ¥æ–­å¼€ä¸”æ­£åœ¨è¿›è¡Œå¯¹è¯ï¼Œå°è¯•é‡è¿
        if (conversationState.isVoiceMode) {
          console.log('ğŸ”„ æ£€æµ‹åˆ°è¿æ¥æ–­å¼€ï¼Œå°è¯•é‡è¿...')
          initWebSocketConnection()
        }
      }
    }, 5000)
  }

  /**
   * åœæ­¢è¿æ¥çŠ¶æ€æ£€æŸ¥
   */
  const stopConnectionCheck = () => {
    if (connectionCheckInterval) {
      clearInterval(connectionCheckInterval)
      connectionCheckInterval = null
      console.log('ğŸ›‘ åœæ­¢è¿æ¥æ£€æŸ¥')
    }
  }

  /**
   * å¤„ç†WebSocketæ¶ˆæ¯
   */
  const handleWebSocketMessage = (message) => {
    const { type, data } = message
    
    switch (type) {
      case 'status':
        console.log('ğŸ“Š æœåŠ¡å™¨çŠ¶æ€:', data)
        break

      case 'conversation_started':
        console.log('ğŸ¯ å¯¹è¯å·²å¼€å§‹:', data)
        conversationState.conversationId = data.conversation_id
        conversationState.currentState = ConversationState.LISTENING
        break

      case 'conversation_stopped':
        console.log('ğŸ›‘ å¯¹è¯å·²åœæ­¢:', data)
        conversationState.conversationId = null
        conversationState.currentState = ConversationState.IDLE
        break

      case 'vad_result':
        handleVADResult(data)
        break

      case 'vad_event':
        handleVADEvent(data)
        break

      case 'turn_detection_event':
        handleTurnDetectionEvent(data)
        break

      case 'asr_result':
        handleASRResult(data)
        break

      case 'llm_response':
        handleLLMResponse(data)
        break

      case 'tts_event':
        handleTTSEvent(data)
        break

      case 'tts_audio':
        handleTTSAudio(data)
        break

      case 'conversation_state_change':
        handleStateChange(data)
        break

      case 'error':
        handleError(data)
        break

      case 'pong':
        console.log('ğŸ’“ æ”¶åˆ°å¿ƒè·³å“åº”')
        break

      case 'processing':
        console.log('â³ æœåŠ¡å™¨å¤„ç†ä¸­:', data.message)
        break

      // TENæ¡†æ¶ä¸“ç”¨æ¶ˆæ¯ç±»å‹
      case 'connection_established':
        console.log('ğŸ”Œ è¿æ¥å·²å»ºç«‹:', data)
        break

      case 'speech_start':
        console.log('ğŸ¤ TEN VAD: æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹')
        conversationState.currentState = ConversationState.RECORDING
        break

      case 'speech_end':
        console.log('ğŸ¤ TEN Turn Detection: è¯­éŸ³ç»“æŸ')
        conversationState.currentState = ConversationState.PROCESSING
        break

      case 'transcription_result':
        console.log('ğŸ“ è½¬å½•ç»“æœ:', data.text)
        handleTranscriptionResult(data)
        break

      case 'tts_start':
        console.log('ğŸ”Š TTSå¼€å§‹æ’­æ”¾:', data.text)
        conversationState.currentState = ConversationState.SPEAKING
        // å›éŸ³æŠ‘åˆ¶ï¼šTTSæ’­æ”¾æœŸé—´ç¦ç”¨VAD
        conversationState.vadEnabled = false
        console.log('ğŸ”‡ å›éŸ³æŠ‘åˆ¶ï¼šTTSæ’­æ”¾æœŸé—´ç¦ç”¨VAD')
        break

      case 'tts_end':
        console.log('ğŸ”Š TTSç”Ÿæˆç»“æŸï¼Œç­‰å¾…éŸ³é¢‘æ’­æ”¾å®Œæˆ')
        conversationState.currentState = ConversationState.LISTENING
        // æ³¨æ„ï¼šä¸è¦ç«‹å³é‡æ–°å¯ç”¨VADï¼Œç­‰å¾…éŸ³é¢‘æ’­æ”¾å®Œæˆåå†å¯ç”¨
        // VADå°†åœ¨éŸ³é¢‘é˜Ÿåˆ—æ’­æ”¾å®Œæˆåè‡ªåŠ¨é‡æ–°å¯ç”¨
        break

      case 'listening_ready':
        console.log('ğŸ¤ å‡†å¤‡æ¥æ”¶ä¸‹ä¸€è½®è¯­éŸ³è¾“å…¥:', data.message)
        conversationState.currentState = ConversationState.LISTENING
        // ç¡®ä¿VADå¤„äºæ¿€æ´»çŠ¶æ€
        conversationState.vadEnabled = true
        // æ˜¾ç¤ºæç¤ºä¿¡æ¯
        ElMessage.info('å‡†å¤‡æ¥æ”¶ä¸‹ä¸€è½®è¯­éŸ³è¾“å…¥ï¼Œè¯·å¼€å§‹è¯´è¯')
        break

      case 'init_response':
        console.log('ğŸ”§ æ”¶åˆ°åˆå§‹åŒ–å“åº”:', data)
        break

      case 'audio_received':
        console.log('ğŸµ éŸ³é¢‘æ•°æ®å·²æ¥æ”¶:', data)
        break

      case 'audio_chunk':
        console.log('ğŸµ æ”¶åˆ°éŸ³é¢‘å—:', data.chunk_id)
        handleAudioChunk(data)
        break

      case 'tts_error':
        console.error('âŒ TTSé”™è¯¯:', data.error)
        ElMessage.error(`TTSæ’­æ”¾å¤±è´¥: ${data.error}`)
        conversationState.currentState = ConversationState.LISTENING
        break

      default:
        console.warn('æœªçŸ¥æ¶ˆæ¯ç±»å‹:', type, data)
    }
  }

  /**
   * å¤„ç†è½¬å½•ç»“æœ
   */
  const handleTranscriptionResult = (data) => {
    const { text, confidence } = data

    // æ›´æ–°è½¬å½•æ–‡æœ¬
    conversationState.finalTranscript = text

    // æ·»åŠ åˆ°å¯¹è¯å†å²
    if (text && text.trim()) {
      conversationState.messages.push({
        id: Date.now(),
        type: 'user',
        content: text,
        timestamp: new Date(),
        confidence: confidence
      })
    }

    console.log('ğŸ“ è½¬å½•å®Œæˆ:', text, 'ç½®ä¿¡åº¦:', confidence)
  }

  // éŸ³é¢‘æ’­æ”¾é˜Ÿåˆ—ç®¡ç†
  let audioQueue = []
  let isPlaying = false
  let nextPlayTime = 0
  let currentGainNode = null

  /**
   * å¤„ç†éŸ³é¢‘å—
   */
  const handleAudioChunk = (data) => {
    const { audio, chunk_id, sample_rate } = data

    try {
      // è§£ç base64éŸ³é¢‘æ•°æ®
      const audioBytes = atob(audio)
      const audioArray = new Int16Array(audioBytes.length / 2)

      for (let i = 0; i < audioArray.length; i++) {
        audioArray[i] = (audioBytes.charCodeAt(i * 2) & 0xFF) |
                       ((audioBytes.charCodeAt(i * 2 + 1) & 0xFF) << 8)
      }

      // æ·»åŠ åˆ°éŸ³é¢‘é˜Ÿåˆ—
      audioQueue.push({
        audioData: audioArray,
        sampleRate: sample_rate || 22050,
        chunkId: chunk_id
      })

      // å¼€å§‹æ’­æ”¾é˜Ÿåˆ—
      if (!isPlaying) {
        playAudioQueue()
      }

      console.log('ğŸµ éŸ³é¢‘å—åŠ å…¥é˜Ÿåˆ—:', chunk_id, 'é˜Ÿåˆ—é•¿åº¦:', audioQueue.length)

    } catch (error) {
      console.error('âŒ éŸ³é¢‘å—å¤„ç†å¤±è´¥:', error)
    }
  }

  /**
   * æ’­æ”¾éŸ³é¢‘é˜Ÿåˆ—
   */
  const playAudioQueue = async () => {
    if (isPlaying || audioQueue.length === 0) {
      return
    }

    isPlaying = true
    console.log('ğŸµ å¼€å§‹æ’­æ”¾éŸ³é¢‘é˜Ÿåˆ—ï¼Œå…±', audioQueue.length, 'ä¸ªéŸ³é¢‘å—')

    try {
      while (audioQueue.length > 0) {
        const audioItem = audioQueue.shift()
        await playAudioChunk(audioItem.audioData, audioItem.sampleRate)

        // æ·»åŠ å°é—´éš”é˜²æ­¢éŸ³é¢‘å—é‡å 
        await new Promise(resolve => setTimeout(resolve, 10))
      }
    } catch (error) {
      console.error('âŒ éŸ³é¢‘é˜Ÿåˆ—æ’­æ”¾å¤±è´¥:', error)
    } finally {
      isPlaying = false
      console.log('ğŸµ éŸ³é¢‘é˜Ÿåˆ—æ’­æ”¾å®Œæˆ')

      // éŸ³é¢‘æ’­æ”¾å®Œæˆåï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°å¯ç”¨VAD
      if (conversationState.currentState === ConversationState.LISTENING) {
        // å»¶è¿Ÿé‡æ–°å¯ç”¨VADï¼Œé¿å…å›éŸ³
        setTimeout(() => {
          if (conversationState.currentState === ConversationState.LISTENING) {
            conversationState.vadEnabled = true
            console.log('ğŸ¤ éŸ³é¢‘æ’­æ”¾å®Œæˆï¼Œé‡æ–°å¯ç”¨VADç›‘å¬')
          }
        }, 1000) // 1ç§’å»¶è¿Ÿ
      }
    }
  }

  /**
   * æ’­æ”¾å•ä¸ªéŸ³é¢‘å—
   */
  const playAudioChunk = (audioData, sampleRate) => {
    return new Promise((resolve, reject) => {
      try {
        if (!audioContext) {
          console.warn('âš ï¸ éŸ³é¢‘ä¸Šä¸‹æ–‡æœªåˆå§‹åŒ–')
          resolve()
          return
        }

        // åˆ›å»ºéŸ³é¢‘ç¼“å†²åŒº
        const buffer = audioContext.createBuffer(1, audioData.length, sampleRate)
        const channelData = buffer.getChannelData(0)

        // è½¬æ¢ä¸ºfloat32æ ¼å¼å¹¶åº”ç”¨éŸ³é‡æ§åˆ¶
        for (let i = 0; i < audioData.length; i++) {
          channelData[i] = (audioData[i] / 32768.0) * 0.7  // é™ä½éŸ³é‡åˆ°70%
        }

        // åˆ›å»ºéŸ³é¢‘æº
        const source = audioContext.createBufferSource()
        source.buffer = buffer

        // åˆ›å»ºå¢ç›ŠèŠ‚ç‚¹ç”¨äºéŸ³é‡æ§åˆ¶
        const gainNode = audioContext.createGain()
        gainNode.gain.setValueAtTime(0.7, audioContext.currentTime)

        // è¿æ¥éŸ³é¢‘èŠ‚ç‚¹
        source.connect(gainNode)
        gainNode.connect(audioContext.destination)

        // è®¡ç®—æ’­æ”¾æ—¶é—´ï¼Œç¡®ä¿éŸ³é¢‘å—æŒ‰é¡ºåºæ’­æ”¾
        const currentTime = audioContext.currentTime
        const startTime = Math.max(currentTime, nextPlayTime)
        const duration = buffer.duration

        // æ›´æ–°ä¸‹æ¬¡æ’­æ”¾æ—¶é—´
        nextPlayTime = startTime + duration

        // æ’­æ”¾éŸ³é¢‘
        source.start(startTime)

        // åœ¨éŸ³é¢‘æ’­æ”¾å®Œæˆåresolve
        source.onended = () => {
          resolve()
        }

        source.onerror = (error) => {
          console.error('âŒ éŸ³é¢‘æºæ’­æ”¾é”™è¯¯:', error)
          reject(error)
        }

      } catch (error) {
        console.error('âŒ éŸ³é¢‘æ’­æ”¾å¤±è´¥:', error)
        reject(error)
      }
    })
  }

  /**
   * åœæ­¢å½“å‰éŸ³é¢‘æ’­æ”¾
   */
  const stopAudioPlayback = () => {
    try {
      // æ¸…ç©ºéŸ³é¢‘é˜Ÿåˆ—
      audioQueue = []
      isPlaying = false
      nextPlayTime = 0

      // åœæ­¢å½“å‰æ’­æ”¾çš„éŸ³é¢‘
      if (currentGainNode) {
        currentGainNode.gain.setValueAtTime(0, audioContext.currentTime)
        currentGainNode = null
      }

      console.log('ğŸ›‘ éŸ³é¢‘æ’­æ”¾å·²åœæ­¢')
    } catch (error) {
      console.error('âŒ åœæ­¢éŸ³é¢‘æ’­æ”¾å¤±è´¥:', error)
    }
  }

  /**
   * åˆå§‹åŒ–éŸ³é¢‘ç³»ç»Ÿ
   */
  const initAudioSystem = async () => {
    try {
      // è¯·æ±‚éº¦å…‹é£æƒé™
      audioStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,  // TEN VADè¦æ±‚16kHz
          channelCount: 1
        }
      })
      
      // åˆå§‹åŒ–éŸ³é¢‘ä¸Šä¸‹æ–‡
      audioContext = new (window.AudioContext || window.webkitAudioContext)({
        sampleRate: 16000
      })
      
      // åˆå§‹åŒ–åª’ä½“å½•åˆ¶å™¨
      mediaRecorder = new MediaRecorder(audioStream, {
        mimeType: 'audio/webm;codecs=opus',
        audioBitsPerSecond: 16000
      })
      
      console.log('ğŸ¤ éŸ³é¢‘ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ')
      
    } catch (error) {
      throw new Error(`éŸ³é¢‘ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: ${error.message}`)
    }
  }
  
  /**
   * åˆå§‹åŒ–VAD (è¯­éŸ³æ´»åŠ¨æ£€æµ‹)
   */
  const initVAD = async () => {
    try {
      // åˆ›å»ºéŸ³é¢‘å¤„ç†èŠ‚ç‚¹
      const source = audioContext.createMediaStreamSource(audioStream)
      
      // åˆ›å»ºScriptProcessorç”¨äºå®æ—¶éŸ³é¢‘å¤„ç†
      // æ³¨æ„: ScriptProcessorNodeå·²å¼ƒç”¨ï¼Œä½†ä¸ºäº†å…¼å®¹æ€§æš‚æ—¶ä¿ç•™
      // TODO: æœªæ¥ç‰ˆæœ¬å°†å‡çº§åˆ°AudioWorkletNode
      vadProcessor = audioContext.createScriptProcessor(1024, 1, 1)
      
      vadProcessor.onaudioprocess = (event) => {
        if (!conversationState.vadEnabled) {
          console.debug('ğŸ”‡ VADæœªå¯ç”¨ï¼Œè·³è¿‡éŸ³é¢‘å¤„ç†')
          return
        }

        const inputBuffer = event.inputBuffer.getChannelData(0)

        // è®¡ç®—éŸ³é¢‘èƒ½é‡ç”¨äºè°ƒè¯•
        const energy = inputBuffer.reduce((sum, sample) => sum + sample * sample, 0) / inputBuffer.length
        if (energy > 0.001) {
          console.debug('ğŸ¤ æ£€æµ‹åˆ°éŸ³é¢‘æ´»åŠ¨ï¼Œèƒ½é‡:', energy.toFixed(6))
        }

        // å‘é€éŸ³é¢‘æ•°æ®åˆ°åç«¯VAD
        if (wsConnection && wsConnection.readyState === WebSocket.OPEN) {
          // å°†Float32Arrayè½¬æ¢ä¸ºInt16Arrayä»¥å‡å°‘æ•°æ®é‡
          const int16Array = new Int16Array(inputBuffer.length)
          for (let i = 0; i < inputBuffer.length; i++) {
            int16Array[i] = Math.max(-32768, Math.min(32767, inputBuffer[i] * 32768))
          }
          
          // è½¬æ¢ä¸ºbase64ç¼–ç 
          const audioBuffer = new ArrayBuffer(int16Array.length * 2)
          const view = new DataView(audioBuffer)
          for (let i = 0; i < int16Array.length; i++) {
            view.setInt16(i * 2, int16Array[i], true) // little endian
          }
          const base64Audio = btoa(String.fromCharCode(...new Uint8Array(audioBuffer)))

          const audioMessage = {
            type: 'audio_data',
            data: {
              audio: base64Audio,
              sample_rate: 16000,
              timestamp: Date.now(),
              conversation_id: conversationState.conversationId
            }
          }

          wsConnection.send(JSON.stringify(audioMessage))
          console.debug('ğŸµ å‘é€éŸ³é¢‘æ•°æ®ï¼Œé•¿åº¦:', base64Audio.length, 'å­—èŠ‚')
        }
      }
      
      source.connect(vadProcessor)
      vadProcessor.connect(audioContext.destination)
      
      console.log('ğŸ”Š VADåˆå§‹åŒ–å®Œæˆ')
      
    } catch (error) {
      throw new Error(`VADåˆå§‹åŒ–å¤±è´¥: ${error.message}`)
    }
  }
  
  /**
   * å¼€å§‹è¯­éŸ³å¯¹è¯
   */
  const startVoiceConversation = async () => {
    try {
      if (!wsConnection || wsConnection.readyState !== WebSocket.OPEN) {
        throw new Error('WebSocketè¿æ¥æœªå»ºç«‹')
      }
      
      // ç”Ÿæˆæ–°çš„å¯¹è¯ID
      conversationState.conversationId = `conv_${Date.now()}`
      conversationState.turnCount = 0
      conversationState.isVoiceMode = true
      
      // å¯åŠ¨VADç›‘å¬
      conversationState.vadEnabled = true
      
      // å‘é€å¼€å§‹å¯¹è¯å‘½ä»¤
      wsConnection.send(JSON.stringify({
        type: 'start_conversation',
        data: {
          conversation_id: conversationState.conversationId,
          mode: 'voice'
        }
      }))
      
      // æ›´æ–°çŠ¶æ€
      updateConversationState(ConversationState.LISTENING)
      
      console.log('ğŸ™ï¸ å¼€å§‹è¯­éŸ³å¯¹è¯:', conversationState.conversationId)
      
      // æ›´æ–°ç»Ÿè®¡
      conversationState.stats.totalConversations++
      
      ElMessage.success('è¯­éŸ³å¯¹è¯å·²å¼€å§‹ï¼Œè¯·å¼€å§‹è¯´è¯')
      
    } catch (error) {
      console.error('âŒ å¼€å§‹è¯­éŸ³å¯¹è¯å¤±è´¥:', error)
      conversationState.error = error.message
      ElMessage.error(`å¼€å§‹å¯¹è¯å¤±è´¥: ${error.message}`)
    }
  }
  
  /**
   * åœæ­¢è¯­éŸ³å¯¹è¯
   */
  const stopVoiceConversation = () => {
    try {
      // åœæ­¢VAD
      conversationState.vadEnabled = false
      
      // å‘é€åœæ­¢å¯¹è¯å‘½ä»¤
      if (wsConnection && wsConnection.readyState === WebSocket.OPEN) {
        wsConnection.send(JSON.stringify({
          type: 'stop_conversation',
          data: {
            conversation_id: conversationState.conversationId
          }
        }))
      }
      
      // é‡ç½®çŠ¶æ€
      conversationState.isVoiceMode = false
      conversationState.conversationId = null
      conversationState.realtimeTranscript = ''
      conversationState.finalTranscript = ''
      
      updateConversationState(ConversationState.IDLE)
      
      console.log('ğŸ›‘ åœæ­¢è¯­éŸ³å¯¹è¯')
      ElMessage.info('è¯­éŸ³å¯¹è¯å·²åœæ­¢')
      
    } catch (error) {
      console.error('âŒ åœæ­¢è¯­éŸ³å¯¹è¯å¤±è´¥:', error)
    }
  }
  
  /**
   * æ‰‹åŠ¨ä¸­æ–­AIå›å¤
   */
  const interruptAIResponse = () => {
    try {
      if (conversationState.currentState === ConversationState.SPEAKING) {
        // å‘é€ä¸­æ–­å‘½ä»¤
        if (wsConnection && wsConnection.readyState === WebSocket.OPEN) {
          wsConnection.send(JSON.stringify({
            type: 'interrupt_response',
            data: {
              conversation_id: conversationState.conversationId,
              timestamp: Date.now()
            }
          }))
        }
        
        // æ›´æ–°ç»Ÿè®¡
        conversationState.stats.totalInterruptions++
        conversationState.interruptionCount++
        
        console.log('âš¡ æ‰‹åŠ¨ä¸­æ–­AIå›å¤')
      }
    } catch (error) {
      console.error('âŒ ä¸­æ–­AIå›å¤å¤±è´¥:', error)
    }
  }
  
  /**
   * å¤„ç†VADç»“æœï¼ˆæ¥è‡ªTENæ¡†æ¶ï¼‰
   */
  const handleVADResult = (data) => {
    const { speech_detected, energy, timestamp } = data

    console.log('ğŸ¤ TEN VADç»“æœ:', { speech_detected, energy })

    // æ›´æ–°çŠ¶æ€
    conversationState.speechDetected = speech_detected

    if (speech_detected) {
      conversationState.silenceDetected = false
      conversationState.lastUserSpeechTime = timestamp

      // å¦‚æœæ­£åœ¨è¯´è¯ï¼Œæ£€æµ‹ä¸­æ–­
      if (conversationState.isSpeaking) {
        interruptAIResponse()
      }
    } else {
      conversationState.silenceDetected = true
    }
  }

  /**
   * å¤„ç†VADäº‹ä»¶
   */
  const handleVADEvent = (data) => {
    const { event_type, timestamp } = data
    
    if (event_type === 'speech_start') {
      conversationState.speechDetected = true
      conversationState.silenceDetected = false
      conversationState.lastUserSpeechTime = timestamp
      
      // å¦‚æœAIæ­£åœ¨è¯´è¯ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ä¸­æ–­
      if (conversationState.currentState === ConversationState.SPEAKING) {
        const timeSinceLastSpeech = timestamp - (conversationState.lastUserSpeechTime || 0)
        if (timeSinceLastSpeech <= conversationState.interruptionThreshold) {
          interruptAIResponse()
        }
      }
      
    } else if (event_type === 'speech_end') {
      conversationState.speechDetected = false
      conversationState.silenceDetected = true
    }
  }
  
  /**
   * å¤„ç†è½®æ¢æ£€æµ‹äº‹ä»¶
   */
  const handleTurnDetectionEvent = (data) => {
    const { turn_state, confidence, text } = data
    
    if (turn_state === 'finished' && confidence > 0.8) {
      // ç”¨æˆ·è¯´è¯ç»“æŸï¼Œå¼€å§‹å¤„ç†
      updateConversationState(ConversationState.PROCESSING)
      conversationState.finalTranscript = text
      conversationState.turnCount++
      
    } else if (turn_state === 'wait') {
      // ç”¨æˆ·å¯èƒ½åœ¨ç­‰å¾…
      updateConversationState(ConversationState.WAITING)
    }
  }
  
  /**
   * å¤„ç†ASRç»“æœ
   */
  const handleASRResult = (data) => {
    const { transcript, confidence, is_final } = data

    console.log('ğŸ¯ ASRç»“æœ:', { transcript, confidence, is_final })

    if (is_final) {
      conversationState.finalTranscript = transcript
      conversationState.realtimeTranscript = ''

      // è½¬æ¢çŠ¶æ€åˆ°å¤„ç†ä¸­
      updateConversationState(ConversationState.PROCESSING)
    } else {
      conversationState.realtimeTranscript = transcript
    }
  }
  
  /**
   * å¤„ç†LLMå“åº”
   */
  const handleLLMResponse = (data) => {
    const { response, conversation_id } = data

    console.log('ğŸ¤– LLMå“åº”:', { response, conversation_id })

    // LLMå“åº”å®Œæˆï¼Œç­‰å¾…TTSäº‹ä»¶æ¥ç®¡ç†çŠ¶æ€
    conversationState.lastAISpeechTime = Date.now()

    // æ³¨æ„ï¼šçŠ¶æ€è½¬æ¢ç°åœ¨ç”±TTSäº‹ä»¶å¤„ç†
    // TTSå¼€å§‹æ—¶ä¼šè½¬æ¢åˆ°SPEAKINGçŠ¶æ€
    // TTSç»“æŸæ—¶ä¼šè½¬æ¢å›LISTENINGçŠ¶æ€
  }
  
  /**
   * å¤„ç†TTSäº‹ä»¶
   */
  const handleTTSEvent = (data) => {
    const { event_type, text, conversation_id } = data

    console.log('ğŸ”Š TTSäº‹ä»¶:', { event_type, text })

    if (event_type === 'tts_start') {
      updateConversationState(ConversationState.SPEAKING)
      conversationState.isSpeaking = true
      console.log('ğŸµ å¼€å§‹TTSæ’­æ”¾')

    } else if (event_type === 'tts_end') {
      conversationState.isSpeaking = false
      updateConversationState(ConversationState.LISTENING)
      console.log('ğŸ”„ TTSæ’­æ”¾å®Œæˆï¼Œå›åˆ°ç›‘å¬çŠ¶æ€')

    } else if (event_type === 'tts_interrupted') {
      conversationState.isSpeaking = false
      conversationState.stats.successfulInterruptions++
      updateConversationState(ConversationState.INTERRUPTED)
      console.log('âš¡ TTSè¢«ä¸­æ–­')

      // çŸ­æš‚å»¶è¿Ÿåè½¬åˆ°ç›‘å¬çŠ¶æ€
      setTimeout(() => {
        updateConversationState(ConversationState.LISTENING)
      }, 100)

    } else if (event_type === 'tts_error') {
      conversationState.isSpeaking = false
      updateConversationState(ConversationState.LISTENING)
      console.error('âŒ TTSæ’­æ”¾é”™è¯¯:', data.error)
    }
  }

  /**
   * å¤„ç†TTSéŸ³é¢‘æ•°æ®
   */
  const handleTTSAudio = (data) => {
    const { audio, format, conversation_id } = data

    try {
      console.log('ğŸµ æ”¶åˆ°TTSéŸ³é¢‘æ•°æ®')

      // è§£ç base64éŸ³é¢‘æ•°æ®
      const audioBytes = atob(audio)
      const audioArray = new Uint8Array(audioBytes.length)
      for (let i = 0; i < audioBytes.length; i++) {
        audioArray[i] = audioBytes.charCodeAt(i)
      }

      // åˆ›å»ºéŸ³é¢‘blob
      const audioBlob = new Blob([audioArray], { type: `audio/${format}` })
      const audioUrl = URL.createObjectURL(audioBlob)

      // æ’­æ”¾éŸ³é¢‘
      const audioElement = new Audio(audioUrl)
      audioElement.play().then(() => {
        console.log('ğŸ”Š TTSéŸ³é¢‘æ’­æ”¾å¼€å§‹')
      }).catch(error => {
        console.error('âŒ TTSéŸ³é¢‘æ’­æ”¾å¤±è´¥:', error)
      })

      // éŸ³é¢‘æ’­æ”¾ç»“æŸåæ¸…ç†URL
      audioElement.addEventListener('ended', () => {
        URL.revokeObjectURL(audioUrl)
        console.log('ğŸ”„ TTSéŸ³é¢‘æ’­æ”¾ç»“æŸ')
      })

    } catch (error) {
      console.error('âŒ TTSéŸ³é¢‘å¤„ç†å¤±è´¥:', error)
    }
  }
  
  /**
   * å¤„ç†çŠ¶æ€å˜åŒ–
   */
  const handleStateChange = (data) => {
    const { new_state, conversation_id, message } = data

    console.log('ğŸ”„ æ”¶åˆ°çŠ¶æ€å˜åŒ–:', { new_state, conversation_id, message })

    // åªå¤„ç†å½“å‰å¯¹è¯çš„çŠ¶æ€å˜åŒ–
    if (conversation_id === conversationState.conversationId) {
      updateConversationState(new_state)

      // å¦‚æœTTSæ’­æ”¾å®Œæˆï¼Œç¡®ä¿æ¢å¤åˆ°ç›‘å¬çŠ¶æ€
      if (new_state === 'listening' && message && message.includes('TTSæ’­æ”¾å®Œæˆ')) {
        console.log('ğŸ”Š TTSæ’­æ”¾å®Œæˆï¼Œæ¢å¤ç›‘å¬çŠ¶æ€')
        conversationState.isSpeaking = false
        conversationState.isListening = true
      }
    }
  }
  
  /**
   * å¤„ç†é”™è¯¯
   */
  const handleError = (data) => {
    const { message } = data
    conversationState.error = message
    console.error('âŒ æœåŠ¡å™¨é”™è¯¯:', message)
    ElMessage.error(message)
  }
  
  /**
   * æ›´æ–°å¯¹è¯çŠ¶æ€
   */
  const updateConversationState = (newState) => {
    const oldState = conversationState.currentState
    conversationState.currentState = newState
    
    // æ›´æ–°ç›¸å…³çŠ¶æ€
    conversationState.isListening = newState === ConversationState.LISTENING
    conversationState.isProcessing = newState === ConversationState.PROCESSING
    conversationState.isSpeaking = newState === ConversationState.SPEAKING
    conversationState.isInterrupted = newState === ConversationState.INTERRUPTED
    
    console.log(`ğŸ”„ çŠ¶æ€è½¬æ¢: ${oldState} -> ${newState}`)
  }
  
  /**
   * è·å–çŠ¶æ€æè¿°
   */
  const getStateDescription = computed(() => {
    switch (conversationState.currentState) {
      case ConversationState.IDLE:
        return 'ç‚¹å‡»å¼€å§‹è¯­éŸ³å¯¹è¯'
      case ConversationState.LISTENING:
        return 'æ­£åœ¨ç›‘å¬ï¼Œè¯·è¯´è¯...'
      case ConversationState.PROCESSING:
        return 'æ­£åœ¨å¤„ç†æ‚¨çš„è¯è¯­...'
      case ConversationState.SPEAKING:
        return 'AIæ­£åœ¨å›å¤...'
      case ConversationState.INTERRUPTED:
        return 'å·²ä¸­æ–­ï¼Œç»§ç»­ç›‘å¬...'
      case ConversationState.WAITING:
        return 'ç­‰å¾…æ‚¨ç»§ç»­è¯´è¯...'
      default:
        return 'æœªçŸ¥çŠ¶æ€'
    }
  })
  
  /**
   * è·å–å¯è§†åŒ–çŠ¶æ€
   */
  const getVisualizationState = computed(() => {
    if (conversationState.speechDetected) return 'recording'
    if (conversationState.isSpeaking) return 'speaking'
    if (conversationState.isListening) return 'listening'
    return 'idle'
  })
  
  /**
   * æ¸…ç†èµ„æº
   */
  const cleanup = () => {
    // åœæ­¢å¯¹è¯
    stopVoiceConversation()

    // åœæ­¢å¿ƒè·³å’Œè¿æ¥æ£€æŸ¥
    stopHeartbeat()
    stopConnectionCheck()

    // å…³é—­WebSocket
    if (wsConnection) {
      wsConnection.close(1000, 'Client cleanup')
      wsConnection = null
    }

    // æ¸…ç†éŸ³é¢‘èµ„æº
    if (vadProcessor) {
      vadProcessor.disconnect()
      vadProcessor = null
    }

    if (audioContext) {
      audioContext.close()
      audioContext = null
    }

    if (audioStream) {
      audioStream.getTracks().forEach(track => track.stop())
      audioStream = null
    }

    console.log('ğŸ§¹ è¯­éŸ³å¯¹è¯èµ„æºæ¸…ç†å®Œæˆ')
  }
  
  return {
    // çŠ¶æ€
    conversationState,
    
    // è®¡ç®—å±æ€§
    getStateDescription,
    getVisualizationState,
    
    // æ–¹æ³•
    initVoiceConversation,
    startVoiceConversation,
    stopVoiceConversation,
    interruptAIResponse,
    cleanup
  }
}
