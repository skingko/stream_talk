"""
LM Studio LLMæ‰©å±•
è¿æ¥LM Studio + Qwen3-30B-A3æ¨¡å‹ï¼Œæ”¯æŒæµå¼å“åº”å’Œæ€è€ƒå†…å®¹è¿‡æ»¤
"""

import asyncio
import json
import logging
import time
from typing import Dict, Any, List, Optional
import aiohttp
from ten import (
    Extension,
    TenEnv,
    Cmd,
    StatusCode,
    CmdResult,
    Data,
)

logger = logging.getLogger(__name__)


class LMStudioLLMExtension(Extension):
    """LM Studio LLMæ‰©å±•"""
    
    def __init__(self, name: str):
        super().__init__(name)
        self.is_initialized = False
        
        # LM Studioé…ç½®
        self.lm_studio_url = "http://localhost:1234/v1/chat/completions"
        self.model_name = "qwen3-30b-a3b"  # æ ¹æ®å®é™…æ¨¡å‹åç§°è°ƒæ•´
        self.max_tokens = 2048
        self.temperature = 0.7
        self.top_p = 0.9
        
        # ä¼šè¯ç®¡ç†
        self.session = None
        self.conversation_history = []
        
        # ç³»ç»Ÿæç¤º
        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ï¼Œåå«å°åŠ©æ‰‹ã€‚è¯·ç”¨è‡ªç„¶ã€å‹å¥½çš„è¯­è°ƒå›å¤ç”¨æˆ·ã€‚
ç‰¹ç‚¹ï¼š
1. å›å¤è¦ç®€æ´æ˜äº†ï¼Œé€‚åˆè¯­éŸ³äº¤äº’
2. è¯­è°ƒè¦äº²åˆ‡è‡ªç„¶ï¼Œåƒæœ‹å‹èŠå¤©ä¸€æ ·
3. é¿å…è¿‡é•¿çš„å›å¤ï¼Œæ§åˆ¶åœ¨50å­—ä»¥å†…
4. å¯ä»¥é€‚å½“ä½¿ç”¨è¯­æ°”è¯ï¼Œè®©å¯¹è¯æ›´ç”ŸåŠ¨
5. æ”¯æŒä¸­æ–‡å¯¹è¯"""
    
    async def on_init(self, ten_env: TenEnv) -> None:
        """åˆå§‹åŒ–æ‰©å±•"""
        try:
            logger.info("ğŸš€ åˆå§‹åŒ–LM Studio LLMæ‰©å±•...")
            
            # åˆ›å»ºHTTPä¼šè¯
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30)
            )
            
            # æµ‹è¯•è¿æ¥
            await self._test_connection()
            
            self.is_initialized = True
            logger.info("âœ… LM Studio LLMæ‰©å±•åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ LM Studio LLMæ‰©å±•åˆå§‹åŒ–å¤±è´¥: {e}")
            self.is_initialized = False
    
    async def on_deinit(self, ten_env: TenEnv) -> None:
        """æ¸…ç†æ‰©å±•"""
        if self.session:
            await self.session.close()
        logger.info("ğŸ›‘ LM Studio LLMæ‰©å±•å·²æ¸…ç†")
    
    async def on_cmd(self, ten_env: TenEnv, cmd: Cmd) -> None:
        """å¤„ç†å‘½ä»¤"""
        cmd_name = cmd.get_name()
        
        if cmd_name == "chat":
            await self._handle_chat_command(ten_env, cmd)
        elif cmd_name == "reset_conversation":
            await self._handle_reset_command(ten_env, cmd)
        else:
            logger.warning(f"æœªçŸ¥å‘½ä»¤: {cmd_name}")
            cmd_result = CmdResult.create(StatusCode.ERROR)
            cmd_result.set_property_string("error", f"æœªçŸ¥å‘½ä»¤: {cmd_name}")
            ten_env.return_result(cmd_result, cmd)
    
    async def _test_connection(self):
        """æµ‹è¯•LM Studioè¿æ¥"""
        try:
            test_payload = {
                "model": self.model_name,
                "messages": [
                    {"role": "user", "content": "æµ‹è¯•è¿æ¥"}
                ],
                "max_tokens": 10,
                "temperature": 0.1
            }
            
            async with self.session.post(
                self.lm_studio_url,
                json=test_payload,
                headers={"Content-Type": "application/json"}
            ) as response:
                if response.status == 200:
                    logger.info("âœ… LM Studioè¿æ¥æµ‹è¯•æˆåŠŸ")
                else:
                    raise Exception(f"è¿æ¥æµ‹è¯•å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                    
        except Exception as e:
            raise Exception(f"LM Studioè¿æ¥å¤±è´¥: {e}")
    
    async def _handle_chat_command(self, ten_env: TenEnv, cmd: Cmd):
        """å¤„ç†èŠå¤©å‘½ä»¤"""
        try:
            # è·å–å‚æ•°
            user_text = cmd.get_property_string("text")
            context = cmd.get_property_from_json("context") or []
            streaming = cmd.get_property_bool("streaming") or True
            
            if not user_text:
                raise ValueError("ç¼ºå°‘ç”¨æˆ·è¾“å…¥æ–‡æœ¬")
            
            logger.info(f"ğŸ¤– å¤„ç†LLMè¯·æ±‚: {user_text}")
            
            # å‡†å¤‡æ¶ˆæ¯
            messages = self._prepare_messages(user_text, context)
            
            # å‘é€è¯·æ±‚
            if streaming:
                await self._handle_streaming_chat(ten_env, cmd, messages, user_text)
            else:
                await self._handle_non_streaming_chat(ten_env, cmd, messages, user_text)
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†èŠå¤©å‘½ä»¤å¤±è´¥: {e}")
            cmd_result = CmdResult.create(StatusCode.ERROR)
            cmd_result.set_property_string("error", str(e))
            ten_env.return_result(cmd_result, cmd)
    
    async def _handle_streaming_chat(self, ten_env: TenEnv, cmd: Cmd, messages: List[Dict], user_text: str):
        """å¤„ç†æµå¼èŠå¤©"""
        try:
            payload = {
                "model": self.model_name,
                "messages": messages,
                "max_tokens": self.max_tokens,
                "temperature": self.temperature,
                "top_p": self.top_p,
                "stream": True
            }

            full_response = ""
            thinking_content = ""
            is_thinking = False
            output_content = ""

            async with self.session.post(
                self.lm_studio_url,
                json=payload,
                headers={"Content-Type": "application/json"}
            ) as response:

                if response.status != 200:
                    raise Exception(f"LM Studioè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")

                async for line in response.content:
                    line = line.decode('utf-8').strip()

                    if line.startswith('data: '):
                        data_str = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€

                        if data_str == '[DONE]':
                            break

                        try:
                            data = json.loads(data_str)
                            if 'choices' in data and len(data['choices']) > 0:
                                delta = data['choices'][0].get('delta', {})
                                content = delta.get('content', '')

                                if content:
                                    full_response += content

                                    # å¤„ç†æ€è€ƒå†…å®¹è¿‡æ»¤
                                    filtered_content = self._filter_thinking_content(content, thinking_content, is_thinking)

                                    if filtered_content['has_output']:
                                        output_content += filtered_content['output']

                                        # å‘é€æµå¼å“åº”ç»™TTSï¼ˆåªå‘é€å®é™…è¾“å‡ºå†…å®¹ï¼‰
                                        await self._send_llm_response(ten_env, filtered_content['output'], False)

                                    # æ›´æ–°çŠ¶æ€
                                    thinking_content = filtered_content['thinking']
                                    is_thinking = filtered_content['is_thinking']

                        except json.JSONDecodeError:
                            continue

            # å‘é€æœ€ç»ˆå“åº”
            if output_content:
                await self._send_llm_response(ten_env, output_content, True)

            # æ›´æ–°å¯¹è¯å†å²ï¼ˆåªä¿å­˜è¾“å‡ºå†…å®¹ï¼‰
            self._update_conversation_history(user_text, output_content or "æˆ‘éœ€è¦æ€è€ƒä¸€ä¸‹...")

            # è¿”å›æˆåŠŸç»“æœ
            cmd_result = CmdResult.create(StatusCode.OK)
            cmd_result.set_property_string("response", output_content)
            ten_env.return_result(cmd_result, cmd)

        except Exception as e:
            logger.error(f"âŒ æµå¼èŠå¤©å¤±è´¥: {e}")
            cmd_result = CmdResult.create(StatusCode.ERROR)
            cmd_result.set_property_string("error", str(e))
            ten_env.return_result(cmd_result, cmd)
    
    async def _handle_non_streaming_chat(self, ten_env: TenEnv, cmd: Cmd, messages: List[Dict], user_text: str):
        """å¤„ç†éæµå¼èŠå¤©"""
        try:
            payload = {
                "model": self.model_name,
                "messages": messages,
                "max_tokens": self.max_tokens,
                "temperature": self.temperature,
                "top_p": self.top_p,
                "stream": False
            }
            
            async with self.session.post(
                self.lm_studio_url,
                json=payload,
                headers={"Content-Type": "application/json"}
            ) as response:
                
                if response.status != 200:
                    raise Exception(f"LM Studioè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                
                result = await response.json()
                
                if 'choices' in result and len(result['choices']) > 0:
                    ai_response = result['choices'][0]['message']['content']
                    
                    # å‘é€å“åº”
                    await self._send_llm_response(ten_env, ai_response, True)
                    
                    # æ›´æ–°å¯¹è¯å†å²
                    self._update_conversation_history(user_text, ai_response)
                    
                    # è¿”å›æˆåŠŸç»“æœ
                    cmd_result = CmdResult.create(StatusCode.OK)
                    cmd_result.set_property_string("response", ai_response)
                    ten_env.return_result(cmd_result, cmd)
                else:
                    raise Exception("LM Studioè¿”å›æ ¼å¼é”™è¯¯")
                    
        except Exception as e:
            logger.error(f"âŒ éæµå¼èŠå¤©å¤±è´¥: {e}")
            cmd_result = CmdResult.create(StatusCode.ERROR)
            cmd_result.set_property_string("error", str(e))
            ten_env.return_result(cmd_result, cmd)
    
    async def _send_llm_response(self, ten_env: TenEnv, text: str, is_final: bool):
        """å‘é€LLMå“åº”"""
        data = Data.create("llm_response")
        data.set_property_string("text", text)
        data.set_property_bool("is_final", is_final)
        data.set_property_int("timestamp", int(time.time() * 1000))
        ten_env.send_data(data)
    
    def _prepare_messages(self, user_text: str, context: List[Dict]) -> List[Dict]:
        """å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨"""
        messages = [{"role": "system", "content": self.system_prompt}]
        
        # æ·»åŠ ä¸Šä¸‹æ–‡
        if context:
            messages.extend(context[-10:])  # æœ€è¿‘10è½®å¯¹è¯
        
        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append({"role": "user", "content": user_text})
        
        return messages
    
    def _filter_thinking_content(self, content: str, current_thinking: str, is_thinking: bool) -> Dict[str, Any]:
        """è¿‡æ»¤æ€è€ƒå†…å®¹ï¼Œåªè¿”å›å®é™…è¾“å‡º"""
        result = {
            'thinking': current_thinking,
            'is_thinking': is_thinking,
            'output': '',
            'has_output': False
        }

        # æ£€æŸ¥æ˜¯å¦å¼€å§‹æ€è€ƒ
        if '<think>' in content:
            result['is_thinking'] = True
            # æå–æ€è€ƒå¼€å§‹å‰çš„å†…å®¹
            parts = content.split('<think>')
            if parts[0]:
                result['output'] = parts[0]
                result['has_output'] = True
            # ä¿å­˜æ€è€ƒå†…å®¹
            if len(parts) > 1:
                result['thinking'] = current_thinking + parts[1]

        # æ£€æŸ¥æ˜¯å¦ç»“æŸæ€è€ƒ
        elif '</think>' in content:
            if is_thinking:
                parts = content.split('</think>')
                # æ·»åŠ åˆ°æ€è€ƒå†…å®¹
                result['thinking'] = current_thinking + parts[0]
                result['is_thinking'] = False
                # æå–æ€è€ƒç»“æŸåçš„å†…å®¹
                if len(parts) > 1 and parts[1]:
                    result['output'] = parts[1]
                    result['has_output'] = True
            else:
                # ä¸åœ¨æ€è€ƒçŠ¶æ€ï¼Œç›´æ¥è¾“å‡º
                result['output'] = content
                result['has_output'] = True

        # æ­£åœ¨æ€è€ƒä¸­
        elif is_thinking:
            result['thinking'] = current_thinking + content
            result['is_thinking'] = True

        # æ­£å¸¸è¾“å‡º
        else:
            result['output'] = content
            result['has_output'] = True

        return result

    def _update_conversation_history(self, user_text: str, ai_response: str):
        """æ›´æ–°å¯¹è¯å†å²"""
        self.conversation_history.append({"role": "user", "content": user_text})
        self.conversation_history.append({"role": "assistant", "content": ai_response})

        # ä¿æŒå†å²é•¿åº¦
        if len(self.conversation_history) > 20:
            self.conversation_history = self.conversation_history[-20:]
    
    async def _handle_reset_command(self, ten_env: TenEnv, cmd: Cmd):
        """å¤„ç†é‡ç½®å¯¹è¯å‘½ä»¤"""
        self.conversation_history.clear()
        logger.info("ğŸ”„ å¯¹è¯å†å²å·²é‡ç½®")
        
        cmd_result = CmdResult.create(StatusCode.OK)
        cmd_result.set_property_string("message", "å¯¹è¯å†å²å·²é‡ç½®")
        ten_env.return_result(cmd_result, cmd)
