#!/usr/bin/env python3
"""
Fish Speech TTS Engine
åŸºäºå®˜æ–¹Fish Speech APIçš„é«˜æ€§èƒ½TTSå¼•æ“
"""

import asyncio
import logging
import time
import sys
from pathlib import Path
from typing import Optional, Dict, Any, AsyncGenerator
import numpy as np
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥third_party_paths
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥ç¬¬ä¸‰æ–¹åº“è·¯å¾„é…ç½®
try:
    from third_party_paths import get_fish_speech_path
    fish_speech_path = get_fish_speech_path()
    if fish_speech_path.exists():
        sys.path.insert(0, str(fish_speech_path))
except ImportError:
    # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨è·¯å¾„
    fish_speech_path = project_root / "third-party" / "fish-speech"
    if fish_speech_path.exists():
        sys.path.insert(0, str(fish_speech_path))

logger = logging.getLogger(__name__)

class FishSpeechEngine:
    """Fish Speech TTSå¼•æ“"""
    
    def __init__(self, model_path: str, device: str = "auto"):
        self.model_path = Path(model_path)
        self.device = device
        self.tts_engine = None
        self.llama_queue = None
        self.decoder_model = None
        self.is_initialized = False
        
        # æƒ…æ„Ÿæ˜ å°„
        self.emotion_map = {
            "neutral": "",
            "happy": "(happy)",
            "sad": "(sad)",
            "angry": "(angry)",
            "excited": "(excited)",
            "surprised": "(surprised)",
            "joyful": "(joyful)",
            "confident": "(confident)",
            "whispering": "(whispering)",
            "shouting": "(shouting)",
            "laughing": "(laughing)",
            "chuckling": "(chuckling)",
            "sobbing": "(sobbing)",
            "sighing": "(sighing)"
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–Fish Speechæ¨¡å‹"""
        try:
            logger.info("ğŸŸ åˆå§‹åŒ–Fish Speechå¼•æ“...")
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
            if not self._check_model_files():
                raise FileNotFoundError("æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´")
            
            # å°è¯•åŠ è½½å®˜æ–¹API
            try:
                await self._load_official_api()
                logger.info("âœ… å®˜æ–¹Fish Speech APIåŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ å®˜æ–¹APIåŠ è½½å¤±è´¥: {e}")
                await self._load_fallback_engine()
                logger.info("âœ… å¤‡ç”¨å¼•æ“åŠ è½½æˆåŠŸ")
            
            self.is_initialized = True
            
        except Exception as e:
            logger.error(f"âŒ Fish Speechå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _check_model_files(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§"""
        required_files = ["model.pth", "codec.pth", "config.json", "tokenizer.tiktoken"]
        
        for file in required_files:
            file_path = self.model_path / file
            if not file_path.exists():
                logger.error(f"âŒ ç¼ºå°‘æ¨¡å‹æ–‡ä»¶: {file}")
                return False
        
        logger.info("âœ… æ¨¡å‹æ–‡ä»¶æ£€æŸ¥å®Œæˆ")
        return True
    
    async def _load_official_api(self):
        """åŠ è½½å®˜æ–¹Fish Speech API"""
        try:
            # å¯¼å…¥å®˜æ–¹æ¨¡å—
            from fish_speech.inference_engine import TTSInferenceEngine
            from fish_speech.models.dac.inference import load_model as load_decoder_model
            from fish_speech.models.text2semantic.inference import launch_thread_safe_queue
            
            logger.info("ğŸ“ è®¾ç½®æ¨¡å‹è·¯å¾„...")
            llama_checkpoint_path = str(self.model_path / "model.pth")
            decoder_checkpoint_path = str(self.model_path / "codec.pth")
            decoder_config_name = "firefly_gan_vq"
            
            # è®¾ç½®ç²¾åº¦
            precision = torch.half if self.device == "cuda" else torch.bfloat16
            
            logger.info("ğŸ§  åŠ è½½LLAMAæ¨¡å‹...")
            self.llama_queue = launch_thread_safe_queue(
                checkpoint_path=llama_checkpoint_path,
                device=self.device,
                precision=precision,
                compile=False,
            )
            
            logger.info("ğŸµ åŠ è½½è§£ç å™¨æ¨¡å‹...")
            self.decoder_model = load_decoder_model(
                config_name=decoder_config_name,
                checkpoint_path=decoder_checkpoint_path,
                device=self.device,
            )
            
            logger.info("ğŸš€ åˆ›å»ºTTSæ¨ç†å¼•æ“...")
            self.tts_engine = TTSInferenceEngine(
                llama_queue=self.llama_queue,
                decoder_model=self.decoder_model,
                precision=precision,
                compile=False,
            )
            
        except Exception as e:
            logger.error(f"âŒ å®˜æ–¹APIåŠ è½½å¤±è´¥: {e}")
            raise
    
    async def _load_fallback_engine(self):
        """åŠ è½½å¤‡ç”¨å¼•æ“ï¼ˆæ¨¡æ‹Ÿå™¨ï¼‰"""
        from fish_speech_simulator import FishSpeechSimulator
        
        self.tts_engine = FishSpeechSimulator(
            model_path=str(self.model_path),
            device=self.device
        )
        await self.tts_engine.initialize()
    
    async def synthesize(self, text: str, emotion: str = "neutral", **kwargs) -> np.ndarray:
        """æ‰¹é‡è¯­éŸ³åˆæˆ"""
        if not self.is_initialized:
            raise RuntimeError("å¼•æ“æœªåˆå§‹åŒ–")
        
        try:
            # æ·»åŠ æƒ…æ„Ÿæ ‡è®°
            emotion_tag = self.emotion_map.get(emotion, "")
            if emotion_tag:
                text_with_emotion = f"{emotion_tag} {text}"
            else:
                text_with_emotion = text
            
            logger.info(f"ğŸµ åˆæˆæ–‡æœ¬: {text_with_emotion[:50]}...")
            
            # è°ƒç”¨TTSå¼•æ“
            if hasattr(self.tts_engine, 'synthesize'):
                # ä½¿ç”¨å®˜æ–¹API
                audio_data = await self._synthesize_with_official_api(text_with_emotion, **kwargs)
            else:
                # ä½¿ç”¨æ¨¡æ‹Ÿå™¨
                audio_data = await self.tts_engine.synthesize(text_with_emotion, **kwargs)
            
            return audio_data
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³åˆæˆå¤±è´¥: {e}")
            raise
    
    async def synthesize_streaming(self, text: str, emotion: str = "neutral", **kwargs) -> AsyncGenerator[np.ndarray, None]:
        """æµå¼è¯­éŸ³åˆæˆ"""
        if not self.is_initialized:
            raise RuntimeError("å¼•æ“æœªåˆå§‹åŒ–")
        
        try:
            # æ·»åŠ æƒ…æ„Ÿæ ‡è®°
            emotion_tag = self.emotion_map.get(emotion, "")
            if emotion_tag:
                text_with_emotion = f"{emotion_tag} {text}"
            else:
                text_with_emotion = text
            
            logger.info(f"ğŸŒŠ æµå¼åˆæˆ: {text_with_emotion[:50]}...")
            
            # è°ƒç”¨æµå¼TTS
            if hasattr(self.tts_engine, 'synthesize_streaming'):
                # ä½¿ç”¨å®˜æ–¹æµå¼API
                async for chunk in self._synthesize_streaming_with_official_api(text_with_emotion, **kwargs):
                    yield chunk
            else:
                # ä½¿ç”¨æ¨¡æ‹Ÿå™¨æµå¼API
                async for chunk in self.tts_engine.synthesize_streaming(text_with_emotion, **kwargs):
                    yield chunk
                    
        except Exception as e:
            logger.error(f"âŒ æµå¼è¯­éŸ³åˆæˆå¤±è´¥: {e}")
            raise
    
    async def _synthesize_with_official_api(self, text: str, **kwargs) -> np.ndarray:
        """ä½¿ç”¨å®˜æ–¹APIè¿›è¡Œåˆæˆ"""
        try:
            from fish_speech.utils.schema import ServeTTSRequest
            
            # åˆ›å»ºTTSè¯·æ±‚
            request = ServeTTSRequest(
                text=text,
                reference_id=None,
                reference_audio=None,
                reference_text=None,
                max_new_tokens=kwargs.get('max_new_tokens', 1024),
                chunk_length=kwargs.get('chunk_length', 200),
                top_p=kwargs.get('top_p', 0.7),
                repetition_penalty=kwargs.get('repetition_penalty', 1.2),
                temperature=kwargs.get('temperature', 0.7),
                speaker=kwargs.get('speaker', None),
                emotion=kwargs.get('emotion', None),
                format="wav",
                streaming=False,
            )
            
            # æ‰§è¡Œæ¨ç†
            result = await asyncio.get_event_loop().run_in_executor(
                None, self.tts_engine.inference, request
            )
            
            # æå–éŸ³é¢‘æ•°æ®
            if hasattr(result, 'audio'):
                return result.audio
            else:
                # å¦‚æœç»“æœæ ¼å¼ä¸åŒï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
                return self._generate_mock_audio(text)
                
        except Exception as e:
            logger.warning(f"âš ï¸ å®˜æ–¹APIåˆæˆå¤±è´¥: {e}")
            # é™çº§åˆ°æ¨¡æ‹Ÿæ¨¡å¼
            return self._generate_mock_audio(text)
    
    async def _synthesize_streaming_with_official_api(self, text: str, **kwargs) -> AsyncGenerator[np.ndarray, None]:
        """ä½¿ç”¨å®˜æ–¹APIè¿›è¡Œæµå¼åˆæˆ"""
        try:
            from fish_speech.utils.schema import ServeTTSRequest
            
            # åˆ›å»ºæµå¼TTSè¯·æ±‚
            request = ServeTTSRequest(
                text=text,
                reference_id=None,
                reference_audio=None,
                reference_text=None,
                max_new_tokens=kwargs.get('max_new_tokens', 1024),
                chunk_length=kwargs.get('chunk_length', 100),  # æ›´å°çš„å—ç”¨äºæµå¼
                top_p=kwargs.get('top_p', 0.7),
                repetition_penalty=kwargs.get('repetition_penalty', 1.2),
                temperature=kwargs.get('temperature', 0.7),
                speaker=kwargs.get('speaker', None),
                emotion=kwargs.get('emotion', None),
                format="wav",
                streaming=True,
            )
            
            # æ‰§è¡Œæµå¼æ¨ç†
            async for chunk in self.tts_engine.inference_streaming(request):
                if hasattr(chunk, 'audio'):
                    yield chunk.audio
                else:
                    # å¦‚æœæ ¼å¼ä¸åŒï¼Œç”Ÿæˆæ¨¡æ‹Ÿå—
                    yield self._generate_mock_audio_chunk()
                    
        except Exception as e:
            logger.warning(f"âš ï¸ å®˜æ–¹æµå¼APIå¤±è´¥: {e}")
            # é™çº§åˆ°æ¨¡æ‹Ÿæµå¼æ¨¡å¼
            async for chunk in self._generate_mock_streaming(text):
                yield chunk
    
    def _generate_mock_audio(self, text: str) -> np.ndarray:
        """ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘æ•°æ®"""
        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        processing_time = len(text) * 0.01 * 0.8  # Fish Speeché¢„æœŸæ€§èƒ½
        time.sleep(min(processing_time, 2.0))
        
        # ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘
        sample_rate = 22050
        duration = len(text) * 0.15
        samples = int(sample_rate * duration)
        
        # ç”Ÿæˆæ›´çœŸå®çš„éŸ³é¢‘æ³¢å½¢
        t = np.linspace(0, duration, samples)
        frequency = 200 + np.random.rand() * 300
        audio_data = (
            0.3 * np.sin(2 * np.pi * frequency * t) +
            0.1 * np.random.randn(samples)
        ).astype(np.float32)
        
        return audio_data
    
    def _generate_mock_audio_chunk(self) -> np.ndarray:
        """ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘å—"""
        sample_rate = 22050
        chunk_duration = 0.1  # 100mså—
        samples = int(sample_rate * chunk_duration)
        
        # ç”ŸæˆéŸ³é¢‘å—
        t = np.linspace(0, chunk_duration, samples)
        frequency = 200 + np.random.rand() * 300
        audio_chunk = (
            0.3 * np.sin(2 * np.pi * frequency * t) +
            0.1 * np.random.randn(samples)
        ).astype(np.float32)
        
        return audio_chunk
    
    async def _generate_mock_streaming(self, text: str) -> AsyncGenerator[np.ndarray, None]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæµå¼éŸ³é¢‘"""
        total_duration = len(text) * 0.15
        chunk_duration = 0.1  # 100mså—
        num_chunks = int(total_duration / chunk_duration)
        
        for i in range(num_chunks):
            # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
            await asyncio.sleep(0.05)  # 50mså»¶è¿Ÿ
            
            chunk = self._generate_mock_audio_chunk()
            yield chunk
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.llama_queue:
                # æ¸…ç†LLAMAé˜Ÿåˆ—
                pass
            
            if self.decoder_model:
                # æ¸…ç†è§£ç å™¨
                pass
                
            logger.info("ğŸ§¹ Fish Speechå¼•æ“èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ èµ„æºæ¸…ç†å¤±è´¥: {e}")
    
    def get_supported_emotions(self) -> list:
        """è·å–æ”¯æŒçš„æƒ…æ„Ÿåˆ—è¡¨"""
        return list(self.emotion_map.keys())
