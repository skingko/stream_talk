#!/usr/bin/env python3
"""
Spark-TTSåŒ…è£…å™¨
åŸºäºmlx-audioå®ç°ï¼Œæ”¯æŒæµå¼è¯­éŸ³åˆæˆå’Œå‚è€ƒéŸ³é¢‘voice cloning
"""

import os
import sys
import logging
import re
import asyncio
import threading
import gc
import time
from typing import Optional, Generator, Dict, Any, List
import numpy as np
import soundfile as sf
from pathlib import Path

# å¯¼å…¥MLXç”¨äºGPUèµ„æºç®¡ç†
try:
    import mlx.core as mx
    MLX_AVAILABLE = True
except ImportError:
    MLX_AVAILABLE = False

logger = logging.getLogger(__name__)

# å…¨å±€é”ï¼Œç¡®ä¿æ¨¡å‹è®¿é—®çš„çº¿ç¨‹å®‰å…¨
_model_lock = threading.Lock()
_global_model_instance = None

class SparkTTSWrapper:
    """Spark-TTSåŒ…è£…å™¨ï¼ŒåŸºäºmlx-audioå®ç°ï¼Œæ”¯æŒæµå¼è¯­éŸ³åˆæˆå’Œå‚è€ƒéŸ³é¢‘voice cloning"""

    def __init__(self, model_path: str = None):
        """
        åˆå§‹åŒ–Spark-TTSåŒ…è£…å™¨

        Args:
            model_path: Spark-TTSæ¨¡å‹è·¯å¾„
        """
        # å°è¯•å¤šä¸ªæ¨¡å‹è·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°ä¿®å¤çš„æ¨¡å‹
        self.model_candidates = [
            model_path or "models/spark-tts/Spark-TTS-0.5B-6bit",  # æœ¬åœ°ä¿®å¤çš„6bitæ¨¡å‹
            "mlx-community/Spark-TTS-0.5B",  # è¿œç¨‹éé‡åŒ–æ¨¡å‹
            "SparkAudio/Spark-TTS-0.5B",  # å®˜æ–¹éé‡åŒ–æ¨¡å‹
        ]
        self.model_path = None
        self.model_loaded = False
        self.sample_rate = 16000  # ä¸ç›´æ¥è°ƒç”¨ä¿æŒä¸€è‡´çš„é‡‡æ ·ç‡

        # æ¸©æŸ”å¥³å£°é…ç½®
        self.voice_settings = {
            "gender": "female",
            "pitch": -2.0,  # é™ä½éŸ³è°ƒï¼Œæ›´æ¸©æŸ”
            "speed": 0.8,   # è°ƒæ•´ä¸ºæ›´è‡ªç„¶çš„è¯­é€Ÿ
            "temperature": 0.7,  # é€‚ä¸­çš„éšæœºæ€§
            "voice": "af_heart"  # æ¸©æŸ”å¥³å£°
        }

        # é»˜è®¤å‚è€ƒéŸ³é¢‘
        self.default_reference_audio = "assets/éŸ³é¢‘æ ·æœ¬1.-æ¸©å©‰å¥³å£°wav.wav"
        self.reference_text = "æˆ‘æ˜¯ä¸€ä¸ªå¿«ä¹çš„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæ´»åŠ©æ‰‹ï¼Œèƒ½å¤Ÿæµç•…çš„è¾“å‡ºè¯­éŸ³è¡¨è¾¾è‡ªå·±çš„æƒ³æ³•ï¼Œæˆ‘æ¯å¤©éƒ½å¾ˆå¼€å¿ƒå¹¶åŠªåŠ›å·¥ä½œï¼Œå¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å“Ÿã€‚"

        # Spark-TTSæ”¯æŒçš„é€Ÿåº¦æ˜ å°„ - åªèƒ½ä½¿ç”¨å›ºå®šçš„5ä¸ªå€¼
        # Spark-TTSå†…éƒ¨SPEED_MAP: {0.0: 'very_low', 0.5: 'low', 1.0: 'moderate', 1.5: 'high', 2.0: 'very_high'}
        # æˆ‘ä»¬å°†ç”¨æˆ·è¾“å…¥çš„é€Ÿåº¦æ˜ å°„åˆ°è¿™5ä¸ªæ”¯æŒçš„å€¼
        self.speed_map = {
            0.0: 0.0,   # ææ…¢ -> very_low
            0.5: 0.5,   # æ…¢ -> low
            1.0: 0.5,   # æ­£å¸¸ -> low (æ›´è‡ªç„¶çš„è¯­é€Ÿ)
            1.5: 1.0,   # å¿« -> moderate
            2.0: 1.5    # æå¿« -> high
        }

        # æŒä¹…åŒ–æ¨¡å‹ç»„ä»¶
        self.persistent_model = None
        self.model_preloaded = False
        self.model_warmup_done = False

        # åˆå§‹åŒ–æ¨¡å‹
        self._initialize_model()

        # é¢„åŠ è½½æ¨¡å‹ä»¥å®ç°æŒä¹…åŒ–æ¨ç†
        self._preload_persistent_model()

        logger.info(f"âœ… Spark-TTSåŒ…è£…å™¨åˆå§‹åŒ–å®Œæˆ")

    def _map_speed(self, speed: float) -> float:
        """å°†ä»»æ„é€Ÿåº¦å€¼æ˜ å°„åˆ°Spark-TTSæ”¯æŒçš„é€Ÿåº¦å€¼"""
        supported_speeds = list(self.speed_map.keys())

        # å¦‚æœé€Ÿåº¦å€¼å·²ç»åœ¨æ”¯æŒåˆ—è¡¨ä¸­ï¼Œç›´æ¥è¿”å›æ˜ å°„å€¼
        if speed in supported_speeds:
            mapped_value = self.speed_map[speed]
            logger.debug(f"ğŸµ é€Ÿåº¦æ˜ å°„: {speed} -> {mapped_value}")
            return mapped_value

        # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ”¯æŒé€Ÿåº¦å€¼
        closest_speed = min(supported_speeds, key=lambda x: abs(x - speed))
        mapped_value = self.speed_map[closest_speed]
        logger.debug(f"ğŸµ é€Ÿåº¦æ˜ å°„: {speed} -> {closest_speed} -> {mapped_value}")
        return mapped_value

    def _initialize_model(self):
        """åˆå§‹åŒ–Spark-TTSæ¨¡å‹"""
        try:
            # å¯¼å…¥mlx-audioæ¨¡å—
            from mlx_audio.tts.generate import generate_audio
            from mlx_audio.tts.utils import load_model, get_model_path

            self.generate_audio = generate_audio
            self.load_model = load_model
            self.get_model_path = get_model_path

            # å°è¯•æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹
            for model_candidate in self.model_candidates:
                try:
                    logger.info(f"ğŸ” å°è¯•æ¨¡å‹: {model_candidate}")
                    # å°è¯•è·å–æ¨¡å‹è·¯å¾„ï¼ˆè¿™ä¼šè§¦å‘ä¸‹è½½æˆ–éªŒè¯ï¼‰
                    model_path = get_model_path(model_candidate)
                    self.model_path = model_candidate
                    logger.info(f"âœ… æ‰¾åˆ°å¯ç”¨æ¨¡å‹: {model_candidate}")
                    break
                except Exception as model_error:
                    logger.warning(f"âš ï¸ æ¨¡å‹ {model_candidate} ä¸å¯ç”¨: {model_error}")
                    continue

            if self.model_path:
                logger.info(f"ğŸ“¦ ä½¿ç”¨Spark-TTSæ¨¡å‹: {self.model_path}")
                self.model_loaded = True
                logger.info("âœ… Spark-TTSæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            else:
                raise RuntimeError("æ‰€æœ‰æ¨¡å‹å€™é€‰éƒ½ä¸å¯ç”¨")

        except Exception as e:
            logger.error(f"âŒ Spark-TTSæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise RuntimeError(f"Spark-TTSæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")

    def _cleanup_gpu_resources(self):
        """æ¸…ç†GPUèµ„æºï¼Œé˜²æ­¢Metalé”™è¯¯"""
        try:
            if MLX_AVAILABLE:
                # ä½¿ç”¨æ–°çš„APIæ¸…ç†GPUç¼“å­˜
                mx.clear_cache()
                logger.debug("ğŸ§¹ GPUç¼“å­˜å·²æ¸…ç†")

            # Pythonåƒåœ¾å›æ”¶
            gc.collect()

            # æ›´é•¿çš„ç­‰å¾…æ—¶é—´ï¼Œè®©GPUèµ„æºå®Œå…¨é‡Šæ”¾
            time.sleep(0.5)

        except Exception as e:
            logger.warning(f"âš ï¸ GPUèµ„æºæ¸…ç†å¤±è´¥: {e}")

    def _safe_model_access(self, operation_func, *args, **kwargs):
        """çº¿ç¨‹å®‰å…¨çš„æ¨¡å‹è®¿é—®"""
        global _global_model_instance, _model_lock

        with _model_lock:
            try:
                # æ¸…ç†ä¹‹å‰çš„GPUèµ„æº
                self._cleanup_gpu_resources()

                # æ‰§è¡Œæ“ä½œ
                result = operation_func(*args, **kwargs)

                # æ“ä½œå®Œæˆåå†æ¬¡æ¸…ç†
                self._cleanup_gpu_resources()

                return result

            except Exception as e:
                logger.error(f"âŒ å®‰å…¨æ¨¡å‹è®¿é—®å¤±è´¥: {e}")
                # å‘ç”Ÿé”™è¯¯æ—¶å¼ºåˆ¶æ¸…ç†
                self._cleanup_gpu_resources()
                raise e

    def _preload_persistent_model(self):
        """é¢„åŠ è½½æŒä¹…åŒ–æ¨¡å‹ï¼Œé¿å…æ¯æ¬¡æ¨ç†æ—¶é‡æ–°åŠ è½½"""
        try:
            if not self.model_loaded:
                logger.warning("âš ï¸ æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè·³è¿‡é¢„åŠ è½½")
                return

            logger.info("ğŸ”„ å¼€å§‹é¢„åŠ è½½æŒä¹…åŒ–æ¨¡å‹...")

            # æ¸…ç†GPUèµ„æºåå†åŠ è½½
            self._cleanup_gpu_resources()

            # ä½¿ç”¨mlx-audioçš„load_modelé¢„åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨strict=Falseå¿½ç•¥é‡åŒ–å‚æ•°ä¸åŒ¹é…
            self.persistent_model = self.load_model(model_path=self.model_path, strict=False)
            self.model_preloaded = True

            logger.info("âœ… æŒä¹…åŒ–æ¨¡å‹é¢„åŠ è½½æˆåŠŸ")

            # æ‰§è¡Œæ¨¡å‹é¢„çƒ­
            self._warmup_persistent_model()

        except Exception as e:
            logger.warning(f"âš ï¸ æŒä¹…åŒ–æ¨¡å‹é¢„åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨æ ‡å‡†æ–¹å¼: {e}")
            self.persistent_model = None
            self.model_preloaded = False
            # é¢„åŠ è½½å¤±è´¥æ—¶ä¹Ÿè¦æ¸…ç†èµ„æº
            self._cleanup_gpu_resources()

    def _warmup_persistent_model(self):
        """é¢„çƒ­æŒä¹…åŒ–æ¨¡å‹ï¼Œå‡å°‘é¦–æ¬¡æ¨ç†å»¶è¿Ÿ"""
        try:
            if not self.model_preloaded:
                return

            logger.info("ğŸ”¥ å¼€å§‹æ¨¡å‹é¢„çƒ­...")

            # ä½¿ç”¨ç®€çŸ­æ–‡æœ¬è¿›è¡Œé¢„çƒ­æ¨ç†
            warmup_text = "ä½ å¥½"
            warmup_audio = self._generate_with_persistent_model(
                text=warmup_text,
                speed=1.0,
                reference_audio=None,
                warmup=True
            )

            if warmup_audio is not None:
                self.model_warmup_done = True
                logger.info("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")
            else:
                logger.warning("âš ï¸ æ¨¡å‹é¢„çƒ­å¤±è´¥ï¼Œä½†ç»§ç»­è¿è¡Œ")

        except Exception as e:
            logger.warning(f"âš ï¸ æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")
            self.model_warmup_done = False

    def _generate_with_persistent_model(self, text: str, speed: float, reference_audio: str, warmup: bool = False) -> Optional[np.ndarray]:
        """ä½¿ç”¨æŒä¹…åŒ–æ¨¡å‹è¿›è¡ŒéŸ³é¢‘ç”Ÿæˆ"""
        try:
            if not self.model_preloaded or self.persistent_model is None:
                # å¦‚æœæŒä¹…åŒ–æ¨¡å‹ä¸å¯ç”¨ï¼Œå›é€€åˆ°æ ‡å‡†æ–¹å¼
                return None

            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
            output_dir = Path("temp_audio")
            output_dir.mkdir(exist_ok=True)

            # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å‰ç¼€
            import time
            file_prefix = f"temp_audio/persistent_{int(time.time() * 1000)}"
            if warmup:
                file_prefix = f"temp_audio/warmup_{int(time.time() * 1000)}"

            # ä½¿ç”¨ä¸ç›´æ¥è°ƒç”¨å®Œå…¨ç›¸åŒçš„å‚æ•°é€»è¾‘
            # ä¸è¿›è¡Œä»»ä½•é€Ÿåº¦æ˜ å°„æˆ–ç»„åˆï¼Œç›´æ¥ä½¿ç”¨ä¼ å…¥çš„é€Ÿåº¦
            final_speed = speed  # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„é€Ÿåº¦ï¼Œä¸åšä»»ä½•ä¿®æ”¹
            logger.info(f"ğŸµ ä½¿ç”¨ç›´æ¥è°ƒç”¨é€»è¾‘: ä¼ å…¥é€Ÿåº¦={speed}, æœ€ç»ˆé€Ÿåº¦={final_speed}")

            # ä½¿ç”¨ä¸ç›´æ¥è°ƒç”¨å®Œå…¨ç›¸åŒçš„å‚æ•°
            self.generate_audio(
                text=text,
                model_path=self.model_path,
                voice=None,
                speed=final_speed,  # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„é€Ÿåº¦
                lang_code='a',
                ref_audio=reference_audio if reference_audio and os.path.exists(reference_audio) else None,
                ref_text=self.reference_text if reference_audio else None,
                file_prefix=file_prefix,
                audio_format="wav",
                join_audio=True,
                play=False,
                verbose=True if warmup else False
            )

            # è¯»å–ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶
            audio_file = Path(f"{file_prefix}.wav")
            if audio_file.exists():
                audio_data, _ = sf.read(str(audio_file))

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                audio_file.unlink()

                # ç¡®ä¿éŸ³é¢‘æ•°æ®æ ¼å¼æ­£ç¡®
                if isinstance(audio_data, np.ndarray):
                    if audio_data.ndim == 2:  # ç«‹ä½“å£°è½¬å•å£°é“
                        audio_data = np.mean(audio_data, axis=1)

                    if not warmup:
                        logger.info(f"âœ… æŒä¹…åŒ–æ¨¡å‹ç”ŸæˆéŸ³é¢‘: {len(audio_data)} æ ·æœ¬")

                    return audio_data.astype(np.float32)

            return None

        except Exception as e:
            if not warmup:
                logger.error(f"âŒ æŒä¹…åŒ–æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
            return None
    
    def synthesize_stream(self,
                         text: str,
                         emotion: str = "neutral",
                         speed: float = 1.0,
                         reference_audio: Optional[str] = None) -> Generator[Dict[str, Any], None, None]:
        """
        æµå¼è¯­éŸ³åˆæˆ - ä½¿ç”¨å‚è€ƒéŸ³é¢‘æ–¹æ³•

        Args:
            text: è¦åˆæˆçš„æ–‡æœ¬
            emotion: æƒ…æ„Ÿæ ‡è®°
            speed: è¯­é€Ÿ
            reference_audio: å‚è€ƒéŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Yields:
            Dict: åŒ…å«éŸ³é¢‘æ•°æ®çš„å­—å…¸
        """
        try:
            if not self.model_loaded:
                raise RuntimeError("Spark-TTSæ¨¡å‹æœªåŠ è½½")

            # ä½¿ç”¨é»˜è®¤çš„æ¸©å©‰å¥³å£°æ ·æœ¬
            if not reference_audio:
                reference_audio = self.default_reference_audio

            yield from self._synthesize_with_reference(text, emotion, speed, reference_audio)

        except Exception as e:
            logger.error(f"âŒ Spark-TTSåˆæˆå¤±è´¥: {e}")
            raise RuntimeError(f"TTSåˆæˆå¤±è´¥: {e}")

    def _synthesize_with_reference(self, text: str, emotion: str, speed: float, reference_audio: str) -> Generator[Dict[str, Any], None, None]:
        """ä½¿ç”¨å‚è€ƒéŸ³é¢‘è¿›è¡Œè¯­éŸ³åˆæˆ - åŸºäºmlx-audioå®ç°"""
        try:
            # å¤„ç†æƒ…æ„Ÿå’Œè¯­æ°”æ ‡è®°
            processed_text = self._process_emotion_markers(text, emotion)
            
            logger.info(f"ğŸµ å¼€å§‹Spark-TTSåˆæˆ: {processed_text[:50]}...")
            
            # å®ç°å¥çº§åˆ‡åˆ†+åˆ†ç»„è¾“å‡ºçš„æµå¼éŸ³é¢‘ç”Ÿæˆ
            sentences = self._split_text_to_sentences(processed_text)
            sentence_groups = self._group_sentences_for_output(sentences)

            logger.info(f"ğŸµ å¥å­åˆ†ç»„: å…±{len(sentences)}ä¸ªå¥å­ï¼Œåˆ†ä¸º{len(sentence_groups)}ç»„")
            if sentence_groups:
                logger.info(f"ğŸµ ç¬¬ä¸€ç»„: {len(sentence_groups[0])}ä¸ªå¥å­ï¼Œåç»­æ¯ç»„1ä¸ªå¥å­")

            for group_index, sentence_group in enumerate(sentence_groups):
                # åˆå¹¶å½“å‰ç»„çš„å¥å­
                combined_text = "".join(sentence_group)
                if not combined_text.strip():
                    continue

                logger.debug(f"ğŸ¶ åˆæˆç¬¬{group_index+1}ç»„: {combined_text[:50]}...")

                # ä½¿ç”¨mlx-audioç”ŸæˆéŸ³é¢‘
                audio_data = self._generate_sentence_audio(combined_text, speed, reference_audio)

                if audio_data is not None:
                    yield {
                        "audio": audio_data,
                        "sample_rate": self.sample_rate,
                        "is_final": group_index == len(sentence_groups) - 1,
                        "group_size": len(sentence_group),
                        "group_index": group_index
                    }

            logger.info(f"âœ… Spark-TTSåˆæˆå®Œæˆï¼Œå…±{len(sentence_groups)}ä¸ªéŸ³é¢‘ç»„")

        except Exception as e:
            logger.error(f"âŒ Spark-TTSåˆæˆå¤±è´¥: {e}")
            raise RuntimeError(f"TTSåˆæˆå¤±è´¥: {e}")

    def _split_text_to_sentences(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºæ–­å¥ï¼Œç”¨äºæµå¼å¤„ç†ï¼ˆæ›´ç»†ç²’åº¦çš„åˆ†å‰²ï¼‰"""
        # ç§»é™¤æƒ…æ„Ÿæ ‡è®°ï¼Œé¿å…å½±å“åˆ†å‰²
        text = re.sub(r'\([^)]*\)', '', text)

        # æ›´ç»†ç²’åº¦çš„æ–­å¥åˆ†å‰²ï¼ŒåŒ…æ‹¬é€—å·ã€åˆ†å·ç­‰
        # ä¸­æ–‡æ–­å¥ï¼šå¥å·ã€æ„Ÿå¹å·ã€é—®å·ã€é€—å·ã€åˆ†å·ã€å†’å·
        sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼Œï¼›ï¼š])', text)

        # è‹±æ–‡æ–­å¥ï¼šå¥å·ã€æ„Ÿå¹å·ã€é—®å·ã€é€—å·ã€åˆ†å·ã€å†’å·
        sentences = [s for sentence in sentences for s in re.split(r'(?<=[.!?,:;])\s+', sentence)]

        # è¿‡æ»¤ç©ºå¥å­å’Œåªæœ‰æ ‡ç‚¹çš„å¥å­
        sentences = [s.strip() for s in sentences if s.strip() and not re.match(r'^[ã€‚ï¼ï¼Ÿï¼Œï¼›ï¼š.!?,:;\s]*$', s.strip())]

        return sentences

    def _group_sentences_for_output(self, sentences: List[str]) -> List[List[str]]:
        """å°†æ–­å¥åˆ†ç»„ç”¨äºè¾“å‡ºï¼šç¬¬ä¸€ç»„3ä¸ªæ–­å¥ï¼Œä¹‹åæ¯ç»„1ä¸ªæ–­å¥"""
        if not sentences:
            return []

        groups = []

        # ç¬¬ä¸€ç»„ï¼šå‰3ä¸ªæ–­å¥ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        first_group_size = min(3, len(sentences))
        if first_group_size > 0:
            groups.append(sentences[:first_group_size])

        # åç»­ç»„ï¼šæ¯ç»„1ä¸ªæ–­å¥
        for i in range(first_group_size, len(sentences)):
            groups.append([sentences[i]])

        return groups

    def _generate_sentence_audio(self, text: str, speed: float, reference_audio: str) -> Optional[np.ndarray]:
        """ç”Ÿæˆå•ä¸ªå¥å­çš„éŸ³é¢‘ - ä¼˜å…ˆä½¿ç”¨æŒä¹…åŒ–æ¨¡å‹"""
        try:
            # æ·»åŠ æ›´é•¿å»¶è¿Ÿä»¥é¿å…Metalé”™è¯¯
            time.sleep(1.0)

            # æ¸…ç†GPUèµ„æº
            self._cleanup_gpu_resources()

            # ä¼˜å…ˆå°è¯•ä½¿ç”¨æŒä¹…åŒ–æ¨¡å‹
            if self.model_preloaded and self.persistent_model is not None:
                audio_data = self._generate_with_persistent_model(text, speed, reference_audio)
                if audio_data is not None:
                    return audio_data
                else:
                    logger.warning("âš ï¸ æŒä¹…åŒ–æ¨¡å‹æ¨ç†å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ–¹å¼")

            # å›é€€åˆ°æ ‡å‡†æ–¹å¼
            return self._generate_with_standard_method(text, speed, reference_audio)

        except Exception as e:
            logger.error(f"âŒ å¥å­éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            # å‘ç”Ÿé”™è¯¯æ—¶æ¸…ç†GPUèµ„æº
            self._cleanup_gpu_resources()
            return None

    def _generate_with_standard_method(self, text: str, speed: float, reference_audio: str) -> Optional[np.ndarray]:
        """ä½¿ç”¨æ ‡å‡†æ–¹æ³•ç”ŸæˆéŸ³é¢‘ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        try:
            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
            output_dir = Path("temp_audio")
            output_dir.mkdir(exist_ok=True)

            # é…ç½®Spark-TTSå‚æ•° - ä½¿ç”¨æ¸©æŸ”å¥³å£°è®¾ç½®
            model_path = self.model_path

            # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å‰ç¼€
            import time
            file_prefix = f"temp_audio/standard_{int(time.time() * 1000)}"

            # ä½¿ç”¨ä¸ç›´æ¥è°ƒç”¨å®Œå…¨ç›¸åŒçš„å‚æ•°é€»è¾‘
            final_speed = speed  # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„é€Ÿåº¦ï¼Œä¸åšä»»ä½•ä¿®æ”¹
            logger.info(f"ğŸµ æ ‡å‡†æ–¹æ³•ä½¿ç”¨ç›´æ¥è°ƒç”¨é€»è¾‘: ä¼ å…¥é€Ÿåº¦={speed}, æœ€ç»ˆé€Ÿåº¦={final_speed}")

            # ä½¿ç”¨æ­£ç¡®çš„APIå‚æ•°è¿›è¡ŒéŸ³é¢‘ç”Ÿæˆ
            def _standard_generate_operation():
                # ä½¿ç”¨ä¸ç›´æ¥è°ƒç”¨å®Œå…¨ç›¸åŒçš„å‚æ•°
                self.generate_audio(
                    text=text,
                    model_path=model_path,
                    voice=None,
                    speed=final_speed,  # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„é€Ÿåº¦
                    lang_code='a',
                    ref_audio=reference_audio if reference_audio and os.path.exists(reference_audio) else None,
                    ref_text=self.reference_text if reference_audio else None,
                    file_prefix=file_prefix,
                    audio_format="wav",
                    join_audio=True,
                    play=False,
                    verbose=False
                )
                return file_prefix

            # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ¨¡å‹è®¿é—®
            file_prefix = self._safe_model_access(_standard_generate_operation)

            # è¯»å–ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶
            audio_file = Path(f"{file_prefix}.wav")
            if audio_file.exists():
                audio_data, _ = sf.read(str(audio_file))

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                audio_file.unlink()

                # ç¡®ä¿éŸ³é¢‘æ•°æ®æ ¼å¼æ­£ç¡®
                if isinstance(audio_data, np.ndarray):
                    if audio_data.ndim == 2:  # ç«‹ä½“å£°è½¬å•å£°é“
                        audio_data = np.mean(audio_data, axis=1)
                    logger.info(f"âœ… æ ‡å‡†æ–¹æ³•ç”ŸæˆéŸ³é¢‘: {len(audio_data)} æ ·æœ¬")
                    return audio_data.astype(np.float32)

            return None

        except Exception as e:
            logger.error(f"âŒ æ ‡å‡†æ–¹æ³•éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return None



    def _process_emotion_markers(self, text: str, emotion: str) -> str:
        """å¤„ç†æƒ…æ„Ÿæ ‡è®°ï¼ŒSpark-TTSä¸æ”¯æŒæƒ…æ„Ÿæ ‡è®°ï¼Œç›´æ¥ç§»é™¤"""
        try:
            # ç§»é™¤æ‰€æœ‰æƒ…æ„Ÿæ ‡è®°ï¼ŒSpark-TTSä¸æ”¯æŒ
            import re
            text = re.sub(r'\([^)]*\)', '', text).strip()

            # æ¸…ç†å¤šä½™çš„ç©ºæ ¼
            text = re.sub(r'\s+', ' ', text).strip()

            logger.debug(f"ğŸ§¹ ç§»é™¤æƒ…æ„Ÿæ ‡è®°å: {text}")
            return text

        except Exception as e:
            logger.error(f"âŒ å¤„ç†æƒ…æ„Ÿæ ‡è®°å¤±è´¥: {e}")
            return text

    def is_model_loaded(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        return self.model_loaded
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_path": self.model_path,
            "model_loaded": self.is_model_loaded(),
            "sample_rate": self.sample_rate,
            "device": "mps" if hasattr(self, 'device') else "auto"
        }

    def configure_voice_settings(self, gender: str = "female", pitch: float = -2.0, speed: float = 1.0):
        """é…ç½®æ¸©æŸ”å¥³å£°è®¾ç½®"""
        self.voice_settings.update({
            "gender": gender,
            "pitch": pitch,
            "speed": speed
        })
        logger.info(f"ğŸ¤ é…ç½®è¯­éŸ³è®¾ç½®: gender={gender}, pitch={pitch}, speed={speed}")

    def synthesize_stream_realtime(self,
                                  text: str,
                                  emotion: str = "neutral",
                                  speed: float = 1.0,
                                  reference_audio: Optional[str] = None) -> Generator[Dict[str, Any], None, None]:
        """
        çœŸæ­£çš„å®æ—¶æµå¼è¯­éŸ³åˆæˆ - åŸºäºå¥çº§åˆ‡åˆ†+å¹¶è¡Œå¤„ç†

        Args:
            text: è¦åˆæˆçš„æ–‡æœ¬
            emotion: æƒ…æ„Ÿæ ‡è®°
            speed: è¯­é€Ÿ
            reference_audio: å‚è€ƒéŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Yields:
            Dict: åŒ…å«éŸ³é¢‘æ•°æ®çš„å­—å…¸
        """
        try:
            if not self.model_loaded:
                raise RuntimeError("Spark-TTSæ¨¡å‹æœªåŠ è½½")

            # ä½¿ç”¨é»˜è®¤çš„æ¸©å©‰å¥³å£°æ ·æœ¬
            if not reference_audio:
                reference_audio = "assets/éŸ³é¢‘æ ·æœ¬1.-æ¸©å©‰å¥³å£°wav.wav"

            # å¤„ç†æƒ…æ„Ÿå’Œè¯­æ°”æ ‡è®°
            processed_text = self._process_emotion_markers(text, emotion)

            logger.info(f"ğŸµ å¼€å§‹Spark-TTSå®æ—¶æµå¼åˆæˆ: {processed_text[:50]}...")

            # å®ç°å¥çº§åˆ‡åˆ†+å¹¶è¡Œæ¨ç†çš„æµå¼éŸ³é¢‘ç”Ÿæˆ
            sentences = self._split_text_to_sentences(processed_text)

            # ä½¿ç”¨ä¸²è¡Œå¤„ç†é¿å…GPUå†²çªï¼Œç¦ç”¨å¹¶å‘
            logger.info("ğŸ”„ ä½¿ç”¨ä¸²è¡Œå¤„ç†é¿å…Metalé”™è¯¯...")

            for i, sentence in enumerate(sentences):
                if sentence.strip():
                    try:
                        # ä¸²è¡Œç”Ÿæˆæ¯ä¸ªå¥å­çš„éŸ³é¢‘
                        audio_data = self._generate_sentence_audio_optimized(sentence, speed, reference_audio)
                        if audio_data is not None:
                            yield {
                                "audio": audio_data,
                                "sample_rate": self.sample_rate,
                                "is_final": i == len(sentences) - 1,
                                "sentence_index": i,
                                "sentence_text": sentence[:30] + "..." if len(sentence) > 30 else sentence
                            }
                        else:
                            logger.warning(f"âš ï¸ å¥å­ {i + 1} ç”Ÿæˆå¤±è´¥")
                    except Exception as e:
                        logger.error(f"âŒ å¥å­ {i + 1} å¤„ç†å¼‚å¸¸: {e}")
                        # å‘ç”Ÿé”™è¯¯æ—¶æ¸…ç†GPUèµ„æº
                        self._cleanup_gpu_resources()
                        continue

            logger.info(f"âœ… Spark-TTSå®æ—¶æµå¼åˆæˆå®Œæˆï¼Œå…±{len(sentences)}ä¸ªå¥å­")

        except Exception as e:
            logger.error(f"âŒ Spark-TTSå®æ—¶æµå¼åˆæˆå¤±è´¥: {e}")
            raise RuntimeError(f"TTSå®æ—¶æµå¼åˆæˆå¤±è´¥: {e}")

    def _generate_sentence_audio_optimized(self, text: str, speed: float, reference_audio: str) -> Optional[np.ndarray]:
        """ä¼˜åŒ–çš„å¥å­éŸ³é¢‘ç”Ÿæˆ - é’ˆå¯¹å®æ—¶æ€§ä¼˜åŒ–"""
        try:
            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
            output_dir = Path("temp_audio")
            output_dir.mkdir(exist_ok=True)

            # é…ç½®Spark-TTSå‚æ•° - é’ˆå¯¹å®æ—¶æ€§ä¼˜åŒ–ï¼Œä½¿ç”¨æ¸©æŸ”å¥³å£°è®¾ç½®
            voice = self.voice_settings["voice"]  # æ¸©æŸ”å¥³å£°
            model_path = self.model_path

            # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å‰ç¼€
            import time
            import threading
            thread_id = threading.get_ident()
            file_prefix = f"temp_audio/spark_tts_{thread_id}_{int(time.time() * 1000000)}"

            # ä½¿ç”¨ä¸ç›´æ¥è°ƒç”¨å®Œå…¨ç›¸åŒçš„å‚æ•°é€»è¾‘
            final_speed = speed  # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„é€Ÿåº¦ï¼Œä¸åšä»»ä½•ä¿®æ”¹
            logger.info(f"ğŸµ å®æ—¶æ–¹æ³•ä½¿ç”¨ç›´æ¥è°ƒç”¨é€»è¾‘: ä¼ å…¥é€Ÿåº¦={speed}, æœ€ç»ˆé€Ÿåº¦={final_speed}")

            # ä½¿ç”¨ä¸ç›´æ¥è°ƒç”¨å®Œå…¨ç›¸åŒçš„å‚æ•°
            self.generate_audio(
                text=text,
                model_path=model_path,
                voice=None,  # ä¸ç›´æ¥è°ƒç”¨ä¿æŒä¸€è‡´
                speed=final_speed,  # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„é€Ÿåº¦
                lang_code='a',
                ref_audio=reference_audio if os.path.exists(reference_audio or "") else None,
                ref_text=self.reference_text,
                file_prefix=file_prefix,
                audio_format="wav",
                join_audio=True,
                play=False,
                verbose=False
            )

            # è¯»å–ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶
            audio_file = Path(f"{file_prefix}.wav")
            if audio_file.exists():
                audio_data, _ = sf.read(str(audio_file))

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                audio_file.unlink()

                # ç¡®ä¿éŸ³é¢‘æ•°æ®æ ¼å¼æ­£ç¡®
                if isinstance(audio_data, np.ndarray):
                    if audio_data.ndim == 2:  # ç«‹ä½“å£°è½¬å•å£°é“
                        audio_data = np.mean(audio_data, axis=1)
                    return audio_data.astype(np.float32)

            return None

        except Exception as e:
            logger.error(f"âŒ ä¼˜åŒ–å¥å­éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return None
