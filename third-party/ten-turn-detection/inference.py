#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
TEN Turn Detector Inference Script

This script provides a simple command-line interface for the TEN Turn Detector,
allowing users to analyze text inputs and determine their conversation state.
"""

import argparse
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="TEN Turn Detector - Analyze conversation states in text inputs"
    )
    parser.add_argument(
        "--system", 
        type=str, 
        default="", 
        help="Optional system prompt to provide context"
    )
    parser.add_argument(
        "--input", 
        type=str, 
        required=True, 
        help="User input text to analyze"
    )
    return parser.parse_args()

def load_model(model_id):
    """
    Load the TEN Turn Detector model and tokenizer on GPU with optimizations.

    Args:
        model_id: HuggingFace model ID

    Returns:
        tuple: (model, tokenizer)
    """
    # ç¡®å®šæœ€ä½³è®¾å¤‡
    if torch.cuda.is_available():
        device = "cuda"
        torch_dtype = torch.bfloat16
        print(f"ğŸš€ ä½¿ç”¨CUDA GPU: {torch.cuda.get_device_name()}")
    elif torch.backends.mps.is_available():
        device = "mps"
        torch_dtype = torch.float16  # MPSæ›´é€‚åˆfloat16
        print("ğŸš€ ä½¿ç”¨Apple Silicon GPU (MPS)")
    else:
        device = "cpu"
        torch_dtype = torch.float32
        print("âš ï¸ ä½¿ç”¨CPUï¼Œæ€§èƒ½å¯èƒ½è¾ƒæ…¢")

    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œä¼˜åŒ–GPUä½¿ç”¨
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        trust_remote_code=True,
        torch_dtype=torch_dtype,
        device_map="auto" if device == "cuda" else None,  # è‡ªåŠ¨è®¾å¤‡æ˜ å°„
        low_cpu_mem_usage=True,  # é™ä½CPUå†…å­˜ä½¿ç”¨
        attn_implementation="flash_attention_2" if device == "cuda" else None  # ä½¿ç”¨Flash Attention
    )

    tokenizer = AutoTokenizer.from_pretrained(
        model_id,
        trust_remote_code=True,
        use_fast=True  # ä½¿ç”¨å¿«é€Ÿåˆ†è¯å™¨
    )

    # æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡ï¼ˆå¦‚æœdevice_mapæ²¡æœ‰å¤„ç†ï¼‰
    if device != "cuda":  # device_map="auto"å·²ç»å¤„ç†äº†CUDAæƒ…å†µ
        model = model.to(device)

    model.eval()

    # å¯ç”¨ç¼–è¯‘ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰
    if hasattr(torch, 'compile') and device == "cuda":
        try:
            model = torch.compile(model, mode="reduce-overhead")
            print("âœ… å¯ç”¨PyTorchç¼–è¯‘ä¼˜åŒ–")
        except Exception as e:
            print(f"âš ï¸ ç¼–è¯‘ä¼˜åŒ–å¤±è´¥: {e}")

    return model, tokenizer

def infer(model, tokenizer, system_prompt, user_input):
    """
    Run inference with the TEN Turn Detector with GPU optimizations.

    Args:
        model: The loaded model
        tokenizer: The loaded tokenizer
        system_prompt: Optional system prompt for context
        user_input: User text to analyze

    Returns:
        str: The detected conversation state
    """
    inf_messages = [{"role":"system", "content":system_prompt}] + [{"role":"user", "content":user_input}]

    # ä½¿ç”¨ä¼˜åŒ–çš„åˆ†è¯
    input_ids = tokenizer.apply_chat_template(
        inf_messages,
        add_generation_prompt=True,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512  # é™åˆ¶æœ€å¤§é•¿åº¦
    )

    # Move input to same device as model
    input_ids = input_ids.to(model.device)

    # ä½¿ç”¨ä¼˜åŒ–çš„æ¨ç†è®¾ç½®
    with torch.no_grad():
        # å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆå¦‚æœæ”¯æŒï¼‰
        if model.device.type == "cuda":
            with torch.cuda.amp.autocast():
                outputs = model.generate(
                    input_ids,
                    max_new_tokens=1,
                    do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç æé«˜é€Ÿåº¦
                    num_beams=1,      # å•æŸæœç´¢
                    pad_token_id=tokenizer.eos_token_id,
                    use_cache=True,   # å¯ç”¨KVç¼“å­˜
                    early_stopping=True
                )
        else:
            outputs = model.generate(
                input_ids,
                max_new_tokens=1,
                do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç æé«˜é€Ÿåº¦
                num_beams=1,      # å•æŸæœç´¢
                pad_token_id=tokenizer.eos_token_id,
                use_cache=True,   # å¯ç”¨KVç¼“å­˜
                early_stopping=True
            )

    response = outputs[0][input_ids.shape[-1]:]
    output = tokenizer.decode(response, skip_special_tokens=True)
    return output

if __name__ == "__main__":
    # Parse arguments
    args = parse_args()
    
    # Model ID on HuggingFace
    model_id = 'TEN-framework/TEN_Turn_Detector'
    
    # Load model and tokenizer
    print(f"Loading model from {model_id}...")
    model, tokenizer = load_model(model_id)
    
    # Run inference
    print(f"Running inference on: '{args.input}'")
    output = infer(model, tokenizer, args.system, args.input)
    
    # Print results
    print("\nResults:")
    print(f"Input: '{args.input}'")
    print(f"Turn Detection Result: '{output}'")
